{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T14:22:56.635846Z",
     "start_time": "2019-09-19T14:22:53.992724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "from guess_language import guess_language\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#import nltk\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from nltk.stem.porter import *\n",
    "import corex as ct\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:22:00.916525Z",
     "start_time": "2019-08-11T20:22:00.911421Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Reduce Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:18:10.950030Z",
     "start_time": "2019-08-14T17:18:10.937714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if (props[col].dtype != object and props[col].dtype != 'datetime64[ns]'):  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \",col)\n",
    "            print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            print(\"dtype after: \",props[col].dtype)\n",
    "            print(\"******************************\")\n",
    "    \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T01:36:27.658879Z",
     "start_time": "2019-08-12T01:35:59.526063Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('~/Desktop/Projects/Project_4/Data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T01:36:27.864213Z",
     "start_time": "2019-08-12T01:36:27.660402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweets = reduce_mem_usage(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T01:36:28.350715Z",
     "start_time": "2019-08-12T01:36:27.873385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweet = tweets[['tweet-id', 'timestamp']]\n",
    "del tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T23:48:18.265144Z",
     "start_time": "2019-08-13T23:48:18.263235Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Prepocessing Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:18:18.684777Z",
     "start_time": "2019-08-14T17:18:18.681015Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Creating a timestamp for each tweet in order to be able to make an hourly count\n",
    "def timestamp(tweet):\n",
    "    \"\"\"\n",
    "    Creating a timestamp for each tweet in order to be able to make an hourly count\n",
    "    \"\"\"\n",
    "    d = defaultdict(str)\n",
    "    x = tweet.timestamp.unique()\n",
    "    for i in x:\n",
    "        y = datetime.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')\n",
    "        timestamp = pd.Timestamp(year = y.year, month = y.month, day = y.day, hour = y.hour) \n",
    "        d[i] = timestamp\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:18:18.913784Z",
     "start_time": "2019-08-14T17:18:18.911049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Count the tweets happening at certain hours\n",
    "def hourlytweets(df):\n",
    "    \"\"\"\n",
    "    Count the tweets happening at certain hours\n",
    "    \"\"\"\n",
    "    df_count = df.groupby(['hours'])['tweet-id'].count()\n",
    "    d = df_count.to_dict()\n",
    "    df['activity'] = df['hours'].map(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:18:19.126574Z",
     "start_time": "2019-08-14T17:18:19.124022Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_pattern(txt, pattern):\n",
    "    r = re.findall(pattern,txt)\n",
    "    for i in r:\n",
    "        txt = txt.replace(i,'')\n",
    "    return txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:18:19.493377Z",
     "start_time": "2019-08-14T17:18:19.490788Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gethashtags(txt):\n",
    "    hashtag_list = []\n",
    "    for i in txt:\n",
    "        r = re.findall(\"#[\\w]*\",i)\n",
    "        hashtag_list.append(r)\n",
    "    #hashtag = np.asarray(hashtag_list)\n",
    "    return hashtag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:18:19.732938Z",
     "start_time": "2019-08-14T17:18:19.730222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Delete duplicates\n",
    "def deleteretweets(df):\n",
    "    df.dropna(inplace = True)\n",
    "    df = df.drop_duplicates(subset = 'text', inplace = False)\n",
    "    df = df[~df.text.str.contains('RT')]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T23:48:35.842174Z",
     "start_time": "2019-08-13T23:48:35.840305Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Calling Preproccesing Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Adding Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T01:38:05.839822Z",
     "start_time": "2019-08-12T01:37:49.031722Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Calling the function to create timestamps\n",
    "d_timestamp= timestamp(tweet)\n",
    "del tweet\n",
    "pkl.dump(d_timestamp, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/timestamps.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T01:39:43.979451Z",
     "start_time": "2019-08-12T01:39:43.203887Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Add timestamps to existing DataFrame\n",
    "tweets = pd.read_csv('~/Desktop/Projects/Project_4/Data/tweets.csv')\n",
    "tweets['hours'] = tweets['timestamp'].map(d_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Computing Hourly Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T01:43:29.485935Z",
     "start_time": "2019-08-12T01:43:20.460029Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Compute hourly tweets\n",
    "df = hourlytweets(tweets)\n",
    "pkl.dump(df, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets_timestamps.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Deleting Retweets marked with RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T01:43:59.896423Z",
     "start_time": "2019-08-12T01:43:59.655898Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df_sin = deleteretweets(df)\n",
    "#pkl.dump(df_sin, open('tweets_no_retweets.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cleaning Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:57:11.623588Z",
     "start_time": "2019-08-14T00:57:04.500690Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets_timestamps.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:58:25.343206Z",
     "start_time": "2019-08-14T00:57:11.625148Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df.text.astype(str)\n",
    "#remove twitter handles (@user)\n",
    "df['clean_tweet']= np.vectorize(remove_pattern)(df['text'], \"@[\\w]*\")\n",
    "print('Removed all @...')\n",
    "\n",
    "#Convert string to lowercase letter\n",
    "df['clean_tweet'] = df.clean_tweet.apply(lambda x: x.lower())\n",
    "print('Converted string to lowercase')\n",
    "\n",
    "#Get Hashtags from tweets\n",
    "df['hashtag'] = gethashtags(df['clean_tweet'])\n",
    "#Eliminate list brackets from hashtags\n",
    "df['hashtag_text'] = df['hashtag'].apply(lambda x: ' '.join(map(str, x)))\n",
    "df['hashtag_text'] = df['hashtag_text'].str.replace(\"#\", \"\")\n",
    "# remove special characters, numbers, punctuations.\n",
    "df['hashtag_text'] = df['hashtag_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "#Remove all hashtags from tweets\n",
    "df['clean_tweet']= np.vectorize(remove_pattern)(df['clean_tweet'], \"#[\\w]*\")\n",
    "print('Removed all #...')\n",
    "\n",
    "#Remove URLs\n",
    "df['clean_tweet']= np.vectorize(remove_pattern)(df['clean_tweet'], \"http\\S+\")\n",
    "\n",
    "# remove special characters, numbers, punctuations. \n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Create a column with hashtags and tweet combined\n",
    "df[\"whole_text\"] = df[\"clean_tweet\"].map(str) + df[\"hashtag_text\"]\n",
    "print('Removed all special characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:58:59.369759Z",
     "start_time": "2019-08-14T00:58:25.344873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df, open('cleaned_tweets.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:59:02.680080Z",
     "start_time": "2019-08-14T00:58:59.371579Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sin = df.drop_duplicates(subset = 'clean_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:59:26.227275Z",
     "start_time": "2019-08-14T00:59:02.681479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df_sin, open('cleaned_tweets_no_duplicates.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T01:49:53.204751Z",
     "start_time": "2019-08-14T01:49:34.235145Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Remove all non-english tweets\n",
    "df['clean_tweet_english'] = df.clean_tweet[~df.clean_tweet.apply(lambda x: guess_language(x) != 'en')]\n",
    "df_english = df.dropna(inplace = False)\n",
    "pkl.dump(df_english, open('clean_tweets_english.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:59:26.256391Z",
     "start_time": "2019-08-14T00:59:26.254638Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df_english = pkl.load(open('clean_tweets_english.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T01:59:28.173361Z",
     "start_time": "2019-08-14T01:59:18.924404Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('cleaned_tweets_no_duplicates.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dates are ranging from 02/26/2016 to 03/28/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T18:30:36.251230Z",
     "start_time": "2019-08-15T18:30:36.249343Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:40:45.650534Z",
     "start_time": "2019-08-15T22:40:45.646751Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stem(df, column):\n",
    "    #Tokenization\n",
    "    tokenized_tweet = df[column].apply(lambda x: x.split())\n",
    "    #Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: ' '.join(x))\n",
    "    return tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:46:34.817611Z",
     "start_time": "2019-08-15T22:42:26.976042Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_english['tweet_stem'] = stem(df_english, 'whole_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:46:34.832555Z",
     "start_time": "2019-08-15T22:46:34.819059Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:46:41.601293Z",
     "start_time": "2019-08-15T22:46:34.833986Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df_english, open('tweets_final.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:20:37.111215Z",
     "start_time": "2019-08-14T00:20:37.109291Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train / Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T02:03:47.778250Z",
     "start_time": "2019-08-14T02:03:45.862292Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "#X_train, X_test = train_test_split(df_english, test_size=0.25, random_state=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T02:00:41.725660Z",
     "start_time": "2019-08-14T02:00:16.014431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pkl.dump(X_train, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets_train.pkl', 'wb'))\n",
    "#pkl.dump(X_test, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:11:47.335882Z",
     "start_time": "2019-08-14T00:11:47.333980Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:46:41.606901Z",
     "start_time": "2019-08-15T22:46:41.603170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorization(train):\n",
    "    \"\"\"Vectorizing the train set\"\"\"\n",
    "    cv = CountVectorizer(stop_words = 'english', max_df = 0.7, min_df = 0.05)\n",
    "    X_train_cv = cv.fit_transform(train)\n",
    "    \n",
    "    #Document term matrix including bigrams\n",
    "    cv2 = CountVectorizer(ngram_range=(1,2), binary=True, stop_words='english',  max_df = 0.7, min_df = 0.05)\n",
    "    X_train_cv2 = cv2.fit_transform(train)\n",
    "    \n",
    "    #df = pd.DataFrame(X_train.toarray(),columns = cv.get_feature_names())\n",
    "    return X_train_cv, X_train_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:26:41.109250Z",
     "start_time": "2019-08-20T19:26:35.462787Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets_final.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:47:44.254292Z",
     "start_time": "2019-08-15T22:46:48.554788Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_cv, X_train_cv2 = vectorization(df['whole_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:47:44.301834Z",
     "start_time": "2019-08-15T22:47:44.255735Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(X_train_cv, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/cv.pkl', 'wb'))\n",
    "pkl.dump(X_train_cv2, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/cv_binary.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T00:50:00.762343Z",
     "start_time": "2019-08-14T00:50:00.760425Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T18:09:21.284057Z",
     "start_time": "2019-08-14T18:09:21.280741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create TF-IDF versions of the Count Vectorizers created earlier in the exercise\n",
    "def tfidf(train):\n",
    "    tfidf1 = TfidfVectorizer(stop_words='english',  max_df = 0.7, min_df = 0.05)\n",
    "    X_train_tfidf1 = tfidf1.fit_transform(train)\n",
    "    #X_test_tfidf1  = tfidf1.transform(X_test)\n",
    "\n",
    "    tfidf2 = TfidfVectorizer(ngram_range=(1,2), binary=True, stop_words='english',  max_df = 0.7, min_df = 0.05)\n",
    "    X_train_tfidf2 = tfidf2.fit_transform(train)\n",
    "    #X_test_tfidf2  = tfidf2.transform(X_test)\n",
    "    return X_train_tfidf1, X_train_tfidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:34:43.210963Z",
     "start_time": "2019-08-14T17:34:43.199556Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets_train.pkl', 'rb'))\n",
    "#df_test = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:36:18.685113Z",
     "start_time": "2019-08-14T17:34:43.212148Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf1, X_train_tfidf2 = tfidf(df['whole_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:36:20.850546Z",
     "start_time": "2019-08-14T17:36:18.686681Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(X_train_tfidf1, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/train_tfidf.pkl', 'wb'))\n",
    "pkl.dump(X_train_tfidf1, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/train_tfidf2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T17:36:42.075134Z",
     "start_time": "2019-08-14T17:36:42.053763Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T18:11:38.930534Z",
     "start_time": "2019-08-14T18:11:38.928608Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:23:34.400903Z",
     "start_time": "2019-08-20T19:23:34.398451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopwords1 = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:23:34.406306Z",
     "start_time": "2019-08-20T19:23:34.402554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopwords1.append('just')\n",
    "stopwords1.append('like')\n",
    "stopwords1.append('com')\n",
    "#stopwords1.append('crypto')\n",
    "#stopwords1.append('cryptocurrency')\n",
    "#stopwords1.append('blockchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:23:34.412223Z",
     "start_time": "2019-08-20T19:23:34.407768Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T18:11:46.792702Z",
     "start_time": "2019-08-14T18:11:46.790819Z"
    },
    "hidden": true
   },
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:23:34.416520Z",
     "start_time": "2019-08-20T19:23:34.413461Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lsa(text):\n",
    "    #Applying Vectorization\n",
    "    cv = CountVectorizer(stop_words = stopwords1, max_df = 0.12, min_df = 0.01)\n",
    "    X_cv = cv.fit_transform(text)\n",
    "    \n",
    "    lsa = TruncatedSVD(3)\n",
    "    doc_topic = lsa.fit_transform(X_cv)\n",
    "    lsa.explained_variance_ratio_\n",
    "    \n",
    "    #Show most common words in topics\n",
    "    display_topics(lsa, cv.get_feature_names(), 10)\n",
    "    \n",
    "    return doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:23:53.791569Z",
     "start_time": "2019-08-20T19:23:34.417645Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_topic = lsa(df['whole_text'])\n",
    "#doc_topic_english = lsa(df_english['whole_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:23:53.810842Z",
     "start_time": "2019-08-20T19:23:53.793157Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(doc_topic.argmax(axis = 1)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:56:48.939551Z",
     "start_time": "2019-08-20T19:56:48.936109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def nmf(text):\n",
    "    #Applying Vectorization\n",
    "    cv = CountVectorizer(stop_words = stopwords1, max_df = 0.12, min_df = 10)\n",
    "    #tfidf = TfidfVectorizer(stop_words=stopwords1,  max_df = 0.15, min_df = 0.02)\n",
    "    X_cv = cv.fit_transform(text)\n",
    "    \n",
    "    nmf_model = NMF(3)\n",
    "    doc_topic = nmf_model.fit_transform(X_cv)\n",
    "    \n",
    "    #Show most common words in topics\n",
    "    display_topics(nmf_model, cv.get_feature_names(), 10)\n",
    "    \n",
    "    return doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:57:19.612178Z",
     "start_time": "2019-08-20T19:56:49.939081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_topic = nmf(df['whole_text'])\n",
    "#doc_topic_english = nmf(df_english['whole_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:58:06.308344Z",
     "start_time": "2019-08-20T19:58:06.304860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_topic[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:57:41.642814Z",
     "start_time": "2019-08-20T19:57:41.622844Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(doc_topic.argmax(axis = 1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:50:01.853637Z",
     "start_time": "2019-08-20T19:50:01.849808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:58:48.509953Z",
     "start_time": "2019-08-20T19:58:48.126771Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Topic'] = pd.Series(doc_topic.argmax(axis = 1))\n",
    "labels = ['Blockchain News', 'Cryptocurrency', 'Market']\n",
    "for i in range(3):\n",
    "    df['Topic'] = df['Topic'].replace(to_replace = float(i), value = labels[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:58:48.842144Z",
     "start_time": "2019-08-20T19:58:48.828761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Blockchain_News_proba'] = doc_topic[:,0]\n",
    "df['Cryptocurrency_proba'] = doc_topic[:,1]\n",
    "df['Market_News_proba'] = doc_topic[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:41:03.548863Z",
     "start_time": "2019-08-20T19:40:31.708262Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "# use unstack()\n",
    "df.groupby(['hours','Topic'])['Topic'].count().unstack().plot(kind = 'bar',ax=ax)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tweet Count')\n",
    "for label in ax.xaxis.get_ticklabels()[::200000]:\n",
    "    label.set_visible(False)\n",
    "plt.show()\n",
    "#plt.xticks(ticks = [0,300000,600000,900000] , labels = ['2016', '2017', '2018', '2019'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T00:27:33.543085Z",
     "start_time": "2019-08-18T00:27:33.489996Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crypto_news = df['text'][df.Topic == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T00:27:33.560327Z",
     "start_time": "2019-08-18T00:27:33.544695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_topic_news = nmf(crypto_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T23:47:35.380119Z",
     "start_time": "2019-08-15T23:47:35.377668Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T19:10:52.278650Z",
     "start_time": "2019-08-14T19:10:52.276895Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T19:53:33.221485Z",
     "start_time": "2019-08-14T19:53:33.217035Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lda(text):\n",
    "    #Applying Vectorization\n",
    "    # Create a CountVectorizer for parsing/counting words\n",
    "    cv = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words=stopwords1, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "    X_cv = cv.fit_transform(text).transpose()\n",
    "    \n",
    "    # Convert sparse matrix of counts to a gensim corpus\n",
    "    corpus = matutils.Sparse2Corpus(X_cv)\n",
    "    \n",
    "    #Create dictionary with row and words\n",
    "    id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "    \n",
    "    lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=5)\n",
    "    lda.print_topics()\n",
    "    \n",
    "    lda_corpus = lda[corpus]\n",
    "    \n",
    "    # Store the documents' topic vectors in a list so we can take a peak\n",
    "    lda_docs = [doc for doc in lda_corpus]\n",
    "    \n",
    "    return lda_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T20:21:12.718724Z",
     "start_time": "2019-08-14T19:53:35.123662Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#doc_topic = lda(df['whole_text'])\n",
    "doc_topic_english = lda(df_english['whole_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T00:15:16.910134Z",
     "start_time": "2019-08-15T00:15:16.906883Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ldask(text):\n",
    "    cv = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words=stopwords1, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "    X_cv = cv.fit_transform(text)\n",
    "    \n",
    "    lda_model = LatentDirichletAllocation(n_components=4,random_state=454)\n",
    "    doc_topic = lda_model.fit_transform(X_cv) \n",
    "    \n",
    "    \n",
    "    return doc_topic, cv, lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T00:57:53.504280Z",
     "start_time": "2019-08-15T00:15:17.973860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#doc_topic = ldask(df['whole_text'])\n",
    "doc_topic_english, cv, lda_model = ldask(df_english['whole_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T01:04:00.525326Z",
     "start_time": "2019-08-15T01:03:51.766479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Show most common words in topics\n",
    "display_topics(lda_model, cv.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T00:03:02.826862Z",
     "start_time": "2019-08-15T00:02:46.132877Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "doc_word = vectorizer.fit_transform(df_english['whole_text'])\n",
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Corex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:52:11.615868Z",
     "start_time": "2019-08-15T22:51:56.376589Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords1, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                             binary=True, max_df = 0.15, min_df = 0.01)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(df['whole_text'])\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:59:08.619576Z",
     "start_time": "2019-08-15T22:58:47.022546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topic_model = ct.Corex(n_hidden=3, words=words, seed=10)\n",
    "topic_model.fit(doc_word, words=words, docs=df.whole_text, \n",
    "                anchors=[['news', 'cryptonews'], \n",
    "                         ['eth'], \n",
    "                         ['price', 'market']],anchor_strength=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:59:08.626555Z",
     "start_time": "2019-08-15T22:59:08.621046Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:59:08.870894Z",
     "start_time": "2019-08-15T22:59:08.628077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Top documents for each model\n",
    "topic_model.get_top_docs(topic=0, n_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T22:59:09.000399Z",
     "start_time": "2019-08-15T22:59:08.872198Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(topic_model.predict(doc_word), columns=['topic'+str(i) for i in range(3)])\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T23:42:08.095134Z",
     "start_time": "2019-08-15T23:42:08.084106Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions.topic0.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = topic_model.predict_proba(doc_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T23:43:56.618687Z",
     "start_time": "2019-08-15T23:43:49.719125Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T17:32:55.115535Z",
     "start_time": "2019-08-20T17:32:55.112603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence): \n",
    "  \n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "  \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence) \n",
    "      \n",
    "    #print(\"Overall sentiment dictionary is : \", sentiment_dict) \n",
    "    #print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\") \n",
    "    #print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\") \n",
    "    #print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\") \n",
    "  \n",
    "    #print(\"Sentence Overall Rated As\", end = \" \") \n",
    "  \n",
    "    # decide sentiment as positive, negative and neutral \n",
    "    #if sentiment_dict['compound'] >= 0.05 : \n",
    "        #print(\"Positive\") \n",
    "  \n",
    "    #elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        #print(\"Negative\") \n",
    "  \n",
    "    #else : \n",
    "        #print(\"Neutral\") \n",
    "    return sentiment_dict['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T17:32:55.304185Z",
     "start_time": "2019-08-20T17:32:55.301051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compoundconversion(value):\n",
    "    \"\"\"\n",
    "    Converting compound sentiment value to positive, neutral or negative\n",
    "    \"\"\" \n",
    "    if value >= 0.05 : \n",
    "        return \"Positive\"\n",
    "  \n",
    "    elif value <= - 0.05 : \n",
    "        return \"Negative\"\n",
    "  \n",
    "    else : \n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T17:35:53.095559Z",
     "start_time": "2019-08-20T17:35:53.089383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def topicactivity(df):\n",
    "    #Add the number of times the same topic and sentiment occured at that specified time (in this case hour)\n",
    "    topics = df.groupby(['hours', 'Topic'])['tweet-id'].count().reset_index()\n",
    "    topics = topics.rename({'tweet-id':'topic_activity_hour'}, axis = 1)\n",
    "    df = df.merge(topics, on = ['hours', 'Topic'], how = 'left')\n",
    "    x = df.groupby(['hours', 'Topic', 'sentiment'])['tweet-id'].count().reset_index()\n",
    "    x = x.rename({'tweet-id':'sentiment_topic_count_hour'}, axis = 1)\n",
    "    df = df.merge(x, on = ['hours', 'Topic', 'sentiment'], how = 'left')\n",
    "    #Add the number of times the same topic and sentiment occured at that specified time (in this case day)\n",
    "    topics_day = df.groupby(['Day', 'Topic'])['tweet-id'].count().reset_index()\n",
    "    topics_day = topics_day.rename({'tweet-id':'topic_activity_day'}, axis = 1)\n",
    "    df = df.merge(topics_day, on = ['Day', 'Topic'], how = 'left')\n",
    "    x_days = df.groupby(['Day', 'Topic', 'sentiment'])['tweet-id'].count().reset_index()\n",
    "    x_days = x_days.rename({'tweet-id':'sentiment_topic_count_day'}, axis = 1)\n",
    "    df = df.merge(x_days, on = ['Day', 'Topic', 'sentiment'], how = 'left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T00:50:55.470532Z",
     "start_time": "2019-08-18T00:50:55.445976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "compound = sentiment_scores(df['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:14:49.106658Z",
     "start_time": "2019-08-20T19:14:49.022360Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:16:33.694160Z",
     "start_time": "2019-08-20T19:16:23.254775Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = np.vectorize(sentiment_scores)(df['whole_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:50:21.033917Z",
     "start_time": "2019-08-20T19:50:02.636625Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweet_sentiment.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:58:42.501327Z",
     "start_time": "2019-08-20T19:58:36.531386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweet_sentiment.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:49:19.519876Z",
     "start_time": "2019-08-20T19:49:19.516307Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T01:07:02.503832Z",
     "start_time": "2019-08-18T01:07:02.470983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.hours.sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:58:58.555334Z",
     "start_time": "2019-08-20T19:58:58.334388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df['sentiment'] = \n",
    "df['sentiment'] = np.vectorize(compoundconversion)(df['sentiment_compound'])\n",
    "#s = compoundconversion(df.iloc[0:1000], 'sentiment_compound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:59:05.523784Z",
     "start_time": "2019-08-20T19:58:58.563026Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "months = df[\"hours\"].dt.month\n",
    "years = df[\"hours\"].dt.year\n",
    "df['Month'] = pd.to_datetime(years.astype(str)+months.astype(str) , format = \"%Y%m\")\n",
    "df['Day'] = df['hours'].apply(lambda x: pd.Timestamp(year =x.year, month = x.month, day = x.day, hour = 0, minute = 0, second = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:59:20.548309Z",
     "start_time": "2019-08-20T19:59:05.525820Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = topicactivity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:59:39.989288Z",
     "start_time": "2019-08-20T19:59:20.550925Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T18:04:07.408859Z",
     "start_time": "2019-08-21T18:03:52.199714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T18:04:07.414675Z",
     "start_time": "2019-08-21T18:04:07.410545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T18:04:25.880586Z",
     "start_time": "2019-08-21T18:04:25.875541Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def addsentiment(df, column):\n",
    "    df_groupby = df.groupby([column, 'sentiment'])['tweet-id'].count().reset_index()\n",
    "    df_negative = df_groupby[df_groupby.sentiment == 'Negative'].rename(columns={'tweet-id':'Negative_this_'+column})\n",
    "    df_neutral = df_groupby[df_groupby.sentiment == 'Neutral'].rename(columns={'tweet-id':'Neutral_this_'+column})\n",
    "    df_positive = df_groupby[df_groupby.sentiment == 'Positive'].rename(columns={'tweet-id':'Positive_this_'+column})\n",
    "    df1 = df.merge(df_negative[[column, 'Negative_this_'+column]], on = column, how = 'left')\n",
    "    df1 = df1.merge(df_neutral[[column, 'Neutral_this_'+column]], on = column, how = 'left')\n",
    "    df1 = df1.merge(df_positive[[column, 'Positive_this_'+column]], on  = column, how = 'left')\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T18:04:31.743305Z",
     "start_time": "2019-08-21T18:04:26.666584Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = addsentiment(df, 'Day')\n",
    "df = addsentiment(df, 'Month')\n",
    "df = addsentiment(df, 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T18:04:33.492505Z",
     "start_time": "2019-08-21T18:04:33.489196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:59:49.693296Z",
     "start_time": "2019-08-20T19:59:49.690172Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def posoverneg(df):\n",
    "    df['pos_neg_ratio_day'] = df['Positive_this_Day']/df['Negative_this_Day']\n",
    "    df['pos_neg_ratio_month'] = df['Positive_this_Month']/df['Negative_this_Month']\n",
    "    df['pos_neg_ratio_hour'] = df['Positive_this_hours']/df['Negative_this_hours']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:59:49.703843Z",
     "start_time": "2019-08-20T19:59:49.695165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = posoverneg(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T18:06:57.901763Z",
     "start_time": "2019-08-21T18:06:38.488643Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T19:49:00.641843Z",
     "start_time": "2019-08-20T19:49:00.632527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_4/Data/tweets.pkl', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
