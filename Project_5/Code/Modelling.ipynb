{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:25:19.580458Z",
     "start_time": "2019-09-18T17:25:19.575909Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:33:02.306257Z",
     "start_time": "2019-09-18T17:32:56.642264Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import string\n",
    "from guess_language import guess_language\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from nltk.stem.porter import *\n",
    "import corex as ct\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#Import modules to collect information missing after scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "requests.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:57:27.569922Z",
     "start_time": "2019-09-13T19:57:27.456255Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Data+Transcripts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocessing Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:58:50.931864Z",
     "start_time": "2019-09-18T17:58:50.928536Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#If more time available, comments could be another good way to analyze topics and importance of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:03.372127Z",
     "start_time": "2019-09-08T11:52:03.327158Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Comments\n",
    "d_comments = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Comments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:03.469293Z",
     "start_time": "2019-09-08T11:52:03.460900Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Comments_Transcript'] = df['Links'].map(d_comments)\n",
    "#df['Comments'] = df['Comments'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:03.615825Z",
     "start_time": "2019-09-08T11:52:03.611072Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert empty lists to string so these rows can be taken out\n",
    "def stringconversion(comments):\n",
    "    if len(comments) == 0:\n",
    "        comments = str(comments)\n",
    "        return comments\n",
    "    else:\n",
    "        return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:03.789838Z",
     "start_time": "2019-09-08T11:52:03.780245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Comments_Transcript'] = df['Comments_Transcript'].apply(lambda x: stringconversion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:03.937261Z",
     "start_time": "2019-09-08T11:52:03.914225Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_comments = df[df.Comments_Transcript != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:04.068493Z",
     "start_time": "2019-09-08T11:52:04.062610Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(df_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:04.241987Z",
     "start_time": "2019-09-08T11:52:04.230269Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def englishcomments(comments):\n",
    "    \"\"\"\n",
    "    Filter comments for english language\n",
    "    \"\"\"\n",
    "    comment_list_overall= []\n",
    "    comment_list_video = []\n",
    "    for comment in comments:\n",
    "        comment_list_video = []\n",
    "        for single_comment in comment:\n",
    "            if guess_language(single_comment) == 'en':\n",
    "                comment_list_video.append(single_comment)\n",
    "        comment_list_overall.append(comment_list_video)\n",
    "    return comment_list_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:13.922287Z",
     "start_time": "2019-09-08T11:52:04.385970Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_comments['Comments_Transcript'] = englishcomments(df_comments['Comments_Transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:13.935969Z",
     "start_time": "2019-09-08T11:52:13.924388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create new dataframe for Comments with Links as index\n",
    "df_comments_new = pd.DataFrame(df_comments['Comments_Transcript'], index = df_comments.Links).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:52:13.962877Z",
     "start_time": "2019-09-08T11:52:05.007Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Ã¤remove empty comments to create dataframe with links as index and comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:57:42.368380Z",
     "start_time": "2019-09-13T19:57:42.351866Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert Views number to integer\n",
    "df['Views']=df.Views.astype('str')\n",
    "df['Views']=df.Views.replace(to_replace='nan', value= 0)\n",
    "df['Views'] = df.Views.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:58:38.247974Z",
     "start_time": "2019-09-13T19:58:38.241907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_pattern(txt, pattern):\n",
    "    try:\n",
    "        r = re.findall(pattern,txt)\n",
    "        for i in r:\n",
    "            txt = txt.replace(i,'')\n",
    "        return txt\n",
    "    except:\n",
    "        return txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:58:38.465464Z",
     "start_time": "2019-09-13T19:58:38.460198Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text1 = text.replace(\"'\",'')\n",
    "    clean_text = re.sub('[%s]'%re.escape(string.punctuation),' ',text1)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:58:38.616231Z",
     "start_time": "2019-09-13T19:58:38.613107Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    text1 = re.sub('\\w*\\d\\w*', ' ',text)\n",
    "    return text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:58:38.818626Z",
     "start_time": "2019-09-13T19:58:38.814239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatization(lemmatizer, text):\n",
    "    new_text = []\n",
    "    bracket = text.split(' ')\n",
    "    for word in bracket:\n",
    "        words = lemmatizer.lemmatize(word)\n",
    "        new_text.append(words)\n",
    "    new_text = ' '.join(new_text)\n",
    "    return new_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T18:54:51.960491Z",
     "start_time": "2019-09-17T18:14:20.163315Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Replace &quot; with quotation marks\n",
    "count =0\n",
    "for index, row in df.iterrows():\n",
    "    count+=1\n",
    "    print(count)\n",
    "    title = row.Title\n",
    "    title=title.replace(\"&quot;\", '\"')\n",
    "    df['Title'].iloc[index]=title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:00:41.866662Z",
     "start_time": "2019-09-13T19:58:39.024852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Clean Transcript to conduct Topic Modelling afterwards\n",
    "#Filter for English videos\n",
    "df['Transcript_model_en'] = df.Transcript[~df.Transcript.apply(lambda x: guess_language(x) != 'en')]\n",
    "df = df.dropna(subset = ['Transcript_model_en'], inplace = False)\n",
    "\n",
    "# remove special characters, numbers, punctuations.\n",
    "df['Transcript_Description'] = df['Transcript_model_en']+' '+df['Description']\n",
    "df['Transcript_Description'] = df['Transcript_Description'].apply(lambda x: str(x))\n",
    "df['Transcript_model'] = df['Transcript_model_en'].apply(lambda x: remove_punctuation(x))\n",
    "df['Transcript_model'] = df['Transcript_model_en'].apply(lambda x: remove_numbers(x))\n",
    "df['Transcript_Description'] = df['Transcript_Description'].apply(lambda x: remove_punctuation(x))\n",
    "df['Transcript_Description'] = df['Transcript_Description'].apply(lambda x: remove_numbers(x))\n",
    "\n",
    "\n",
    "#Convert Transcripts to lower string\n",
    "df['Transcript_low'] = df.Transcript_model.apply(lambda x: x.lower())\n",
    "df['Transcript_Description'] = df.Transcript_Description.apply(lambda x: x.lower())\n",
    "print('Converted string to lowercase')\n",
    "\n",
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Transcript_lemmatized'] = df.Transcript_low.apply(lambda x: lemmatization(lemmatizer, x))\n",
    "df['Transcript_Description_lemmatized'] = df.Transcript_Description.apply(lambda x: lemmatization(lemmatizer,x))\n",
    "\n",
    "#Tokenization\n",
    "df['Transcript_sent'] = df.Transcript_low.apply(lambda x: list(ngrams(word_tokenize(x),3)))\n",
    "\n",
    "# remove special characters, numbers, punctuations.\n",
    "#df['Transcript_model'] = df['Transcript_low'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "#df['Transcript_low']= np.vectorize(remove_pattern)(df['Transcripts'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:00:41.922739Z",
     "start_time": "2019-09-13T20:00:41.870962Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Transcript'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:00:41.936297Z",
     "start_time": "2019-09-13T20:00:41.926783Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def slicedate(date):\n",
    "    index = date.find('T')\n",
    "    if index != -1:\n",
    "        date1 = date[:index]\n",
    "        return date1\n",
    "    else:\n",
    "        date1 = 0\n",
    "    return date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:00:41.965371Z",
     "start_time": "2019-09-13T20:00:41.945774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def replacenan(dates):\n",
    "    list_date = []\n",
    "    for date in dates:\n",
    "        if re.findall(r'\\d+', date) == []:\n",
    "            date = 0\n",
    "        list_date.append(date)  \n",
    "    return list_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:00:41.991204Z",
     "start_time": "2019-09-13T20:00:41.970165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convertdates(dates):\n",
    "    \"\"\"\n",
    "    Converting Month strings to a numerical string\n",
    "    \"\"\"\n",
    "    list_dates = []\n",
    "    months = {\n",
    "        'January' : 1,\n",
    "        'February' : 2,\n",
    "        'March' : 3,\n",
    "        'April' : 4,\n",
    "        'May' : 5,\n",
    "        'June' : 6,\n",
    "        'July' : 7,\n",
    "        'August' : 8,\n",
    "        'September' : 9, \n",
    "        'October' : 10,\n",
    "        'November' : 11,\n",
    "        'December' : 12\n",
    "}\n",
    "    for date in dates:\n",
    "        if date != 0:\n",
    "            space_index = date.find(' ')\n",
    "            month = date[:space_index].strip()\n",
    "            try: \n",
    "                new_month = months[month]\n",
    "                year = date[space_index:].strip()\n",
    "                if new_month < 10:\n",
    "                    date = year+'-0'+str(new_month)+'-01'\n",
    "                else:\n",
    "                    date = year+'-'+str(new_month)+'-01'\n",
    "            except:\n",
    "                date = 0\n",
    "        else:\n",
    "            date = 0\n",
    "        list_dates.append(date)\n",
    "    return list_dates\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:00:42.483623Z",
     "start_time": "2019-09-13T20:00:42.004854Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Slice date of Upload Date\n",
    "df['Upload Date1'] = df['Upload Date'].apply(lambda x: slicedate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:01:04.353202Z",
     "start_time": "2019-09-13T20:01:04.344282Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Creating a timestamp for each tweet in order to be able to make an hourly count\n",
    "def timestamp(tweet):\n",
    "    \"\"\"\n",
    "    Creating a timestamp for each tweet in order to be able to make an hourly count\n",
    "    \"\"\"\n",
    "    d = defaultdict(str)\n",
    "    for i in tweet:\n",
    "        if i != 0:\n",
    "            y = datetime.datetime.strptime(i, '%Y-%m-%d')\n",
    "            timestamp = pd.Timestamp(year = y.year, month = y.month, day = y.day) \n",
    "        else:\n",
    "            timestamp =0\n",
    "        d[i] = timestamp\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Event Date'] = df['Event Date'].apply(lambda x: str(x))\n",
    "df['Event Date'] = replacenan(df['Event Date'])\n",
    "\n",
    "#Convert Month into number\n",
    "df['Event Date1'] = convertdates(df['Event Date'])\n",
    "\n",
    "#Convert Event Dates to Timestamp\n",
    "d_timestamp = timestamp(df['Event Date1'])\n",
    "df['Event Date Numerical'] = df['Event Date1'].map(d_timestamp)\n",
    "\n",
    "df.drop(labels = 'Event Date1', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:01:14.184710Z",
     "start_time": "2019-09-13T20:01:13.716428Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert Upload Date to Timestamps\n",
    "d_timestamp = timestamp(df['Upload Date1'])\n",
    "df['Timestamp'] = df['Upload Date1'].map(d_timestamp)\n",
    "df.drop(labels='Upload Date1', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:01:14.335925Z",
     "start_time": "2019-09-13T20:01:14.330421Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def replacezeros(df, column1, column2):\n",
    "    df1 = df\n",
    "    for i in range(len(df1)):\n",
    "        if df1[column1].iloc[i] == 0:\n",
    "            df1[column1].iloc[i] = df1[column2].iloc[i]\n",
    "    return df1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:02:42.624299Z",
     "start_time": "2019-09-13T20:01:16.115609Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = replacezeros(df,'Event Date Numerical', 'Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:55:57.263876Z",
     "start_time": "2019-09-08T11:55:57.256643Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#123 views \n",
    "#123 length\n",
    "#120 Description\n",
    "#120 Author\n",
    "#136 Profession\n",
    "#5 Rating\n",
    "#121Languages\n",
    "#571 Comments\n",
    "#207 Event Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:07:23.916652Z",
     "start_time": "2019-09-13T20:07:23.661896Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:07:40.079450Z",
     "start_time": "2019-09-13T20:07:29.278300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df,open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Model_df.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:27:16.669389Z",
     "start_time": "2019-09-04T17:27:16.660070Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T21:54:15.375175Z",
     "start_time": "2019-09-15T21:54:15.352668Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopwords1 = stopwords.words('english')\n",
    "stopwords1.append('just')\n",
    "stopwords1.append('like')\n",
    "stopwords1.append('com')\n",
    "stopwords1.append('great')\n",
    "stopwords1.append('might')\n",
    "stopwords1.append('talk')\n",
    "stopwords1.append('went')\n",
    "stopwords1.append('ca')\n",
    "stopwords1.append('laughter')\n",
    "stopwords1.append('actually')\n",
    "stopwords1.append('fact')\n",
    "stopwords1.append('nan')\n",
    "stopwords1.append('ok')\n",
    "stopwords1.append('yeah')\n",
    "stopwords1.append('quot')\n",
    "stopwords1.append('wa')\n",
    "stopwords1.append('ha')\n",
    "stopwords1.append('see')\n",
    "stopwords1.append('well')\n",
    "stopwords1.append('would')\n",
    "stopwords1.append('thats')\n",
    "stopwords1.append('thing')\n",
    "stopwords1.append('look')\n",
    "stopwords1.append('im')\n",
    "stopwords1.append('one')\n",
    "stopwords1.append('get')\n",
    "stopwords1.append('know')\n",
    "stopwords1.append('think')\n",
    "stopwords1.append('two')\n",
    "stopwords1.append('say')\n",
    "stopwords1.append('dont')\n",
    "stopwords1.append('felt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T13:55:35.198954Z",
     "start_time": "2019-09-17T13:55:35.055093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(stopwords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T21:54:16.345071Z",
     "start_time": "2019-09-15T21:54:16.338916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Latent Semantic Analysis(LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:36:31.843089Z",
     "start_time": "2019-09-05T15:36:31.825221Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lsa(text):\n",
    "    #Applying Vectorization\n",
    "    cv = CountVectorizer(stop_words = stopwords1, max_df = 0.5, min_df = 0.05)\n",
    "    X_cv = cv.fit_transform(text)\n",
    "    \n",
    "    lsa = TruncatedSVD(10)\n",
    "    doc_topic = lsa.fit_transform(X_cv)\n",
    "    lsa.explained_variance_ratio_\n",
    "    \n",
    "    #Show most common words in topics\n",
    "    display_topics(lsa, cv.get_feature_names(), 10)\n",
    "    \n",
    "    return doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:36:36.794285Z",
     "start_time": "2019-09-05T15:36:32.366963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_topic = lsa(df['Transcript_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T21:59:21.016825Z",
     "start_time": "2019-09-15T21:59:21.012753Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Non-Negative Matrix Factorization (NMF)\n",
    "def nmf(text):\n",
    "    #Applying Vectorization\n",
    "    tfidf = TfidfVectorizer(stop_words = stopwords1, max_df = 0.21, min_df = 0.01)\n",
    "    X_cv = tfidf.fit_transform(text)\n",
    "    \n",
    "    nmf = NMF(32)\n",
    "    doc_topic = nmf.fit_transform(X_cv)\n",
    "    #nmf.explained_variance_ratio_\n",
    "    \n",
    "    #Show most common words in topics\n",
    "    display_topics(nmf, tfidf.get_feature_names(), 10)\n",
    "    \n",
    "    return doc_topic, nmf, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T21:59:35.411652Z",
     "start_time": "2019-09-15T21:59:21.449636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_topic, nmf1, tfidf = nmf(df['Transcript_Description_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:05:12.624853Z",
     "start_time": "2019-09-15T22:05:12.612399Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(doc_topic.argmax(axis = 1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T20:36:13.485077Z",
     "start_time": "2019-09-13T20:36:13.474670Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.Topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T16:04:07.020660Z",
     "start_time": "2019-09-10T16:04:07.008842Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:13:48.725818Z",
     "start_time": "2019-09-17T14:13:47.266139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Topic'] = pd.Series(doc_topic.argmax(axis = 1))\n",
    "labels=['Religion', 'Neuroscience', 'Solar System', 'Music', 'Medical Treatment','Human Biology', 'Economy',\n",
    "      'Astrophysics', 'Robotics', 'Politics','Oceanography', 'Education','Africa', 'Linguistics','War','Racial Discrimination',\n",
    "      'Art','Climate Change','Internet','Cancer','Transportation','Architecture/Design','Sex','Family','Artificial Intelligence',\n",
    "      'E-Sports','Asia','Virology','Children','Ecosystem','Recovery','Criminality']\n",
    "for i in range(32):\n",
    "    df['Topic'] = df['Topic'].replace(to_replace = float(i), value = labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:07:49.465444Z",
     "start_time": "2019-09-15T22:07:49.458584Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels=['Religion', 'Neuroscience', 'Solar System', 'Music', 'Medical Treatment','Human Biology', 'Economy',\n",
    "      'Astrophysics', 'Robotics', 'Politics','Oceanography', 'Education','Africa', 'Linguistics','War','Racial Discrimination',\n",
    "      'Art','Climate Change','Internet','Cancer','Transportation','Architecture/Design','Sex','Family','Artificial Intelligence',\n",
    "      'E-Sports','Asia','Virology','Children','Ecosystem','Recovery','Criminality']\n",
    "display_topics(nmf1, tfidf.get_feature_names(), 10, topic_names = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:29:43.921707Z",
     "start_time": "2019-09-15T22:29:35.566918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df,open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/df_Topics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T15:48:36.752507Z",
     "start_time": "2019-09-15T15:48:34.460915Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/df_Topics.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:25:17.714914Z",
     "start_time": "2019-09-15T22:25:17.645974Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_dates = df[df.Timestamp != 0]\n",
    "df['Year']=df_dates.Timestamp.apply(lambda x: pd.Timestamp(year=x.year, month=1, day=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:13:56.990144Z",
     "start_time": "2019-09-17T14:13:56.854441Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_y=df.groupby(['Year','Topic'])['Topic'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:13:57.197335Z",
     "start_time": "2019-09-17T14:13:57.120763Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_y = df_y.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:13:57.329811Z",
     "start_time": "2019-09-17T14:13:57.308686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_y= df_y.replace(to_replace = np.NaN, value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:13:57.688493Z",
     "start_time": "2019-09-17T14:13:57.541604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:14:01.580425Z",
     "start_time": "2019-09-17T14:14:01.545188Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_y.to_csv('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Topics_over_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:14:06.393719Z",
     "start_time": "2019-09-17T14:14:02.134026Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_y.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Corex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:02:44.121137Z",
     "start_time": "2019-09-05T16:02:39.658322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords1, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                             binary=True, max_df = 0.4, min_df = 0.01)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(df['Transcript_lemmatized'])\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:06:36.980045Z",
     "start_time": "2019-09-05T16:06:27.703993Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topic_model = ct.Corex(n_hidden=9, words=words, seed=10)\n",
    "topic_model.fit(doc_word, words=words, docs=df.Transcript_lemmatized, \n",
    "                anchors=[['education'], \n",
    "                         ['politics', 'government'], \n",
    "                         ['medicine', 'health'],\n",
    "                        ['ocean','earth'],\n",
    "                        ['society'],\n",
    "                        ['space', 'planet','star'],\n",
    "                        ['energy', 'solar'],\n",
    "                        ['country'],\n",
    "                        ['city', 'urban']],anchor_strength=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:06:37.013691Z",
     "start_time": "2019-09-05T16:06:36.982305Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Top documents for each model\n",
    "topic_model.get_top_docs(topic=0, n_docs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T16:04:07.273834Z",
     "start_time": "2019-09-15T16:04:07.269029Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def report_error(message, value):\n",
    "    \"\"\"\n",
    "    Returns: An error message about the given value.\n",
    "\n",
    "    This is a function for constructing error messages to be used in assert statements.\n",
    "    We find that students often introduce bugs into their assert statement messages, and\n",
    "    do not find them because they are in the habit of not writing tests that violate\n",
    "    Preconditions.\n",
    "\n",
    "    The purpose of this function is to give you an easy way of making error messages\n",
    "    without having to worry about introducing such bugs. Look at the function\n",
    "    draw_two_lines for the proper way to use it.\n",
    "\n",
    "    Parameter message: The error message to display\n",
    "    Precondition: message is a string\n",
    "\n",
    "    Parameter value: The value that caused the error\n",
    "    Precondition: NONE (value can be anything)\n",
    "    \"\"\"\n",
    "    return message+': '+repr(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T16:04:07.467742Z",
     "start_time": "2019-09-15T16:04:07.461272Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleaninput(text):\n",
    "    \"\"\"\n",
    "    Clean text given by user\n",
    "    \"\"\"\n",
    "    assert type(text) == str, report_error('Input is not a string', text)\n",
    "    \n",
    "    #Check for english input texxt\n",
    "    if guess_language(text) != 'en':\n",
    "        print('Please provide a proper english text as input')\n",
    "        return\n",
    "    text_new = remove_punctuation(text)\n",
    "    text_new = remove_numbers(text_new)\n",
    "    text_lower = text_new.lower()\n",
    "    \n",
    "    #Lemmatize text\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemmatized = [lemmatization(lemmatizer, i) for i in tokens]\n",
    "    text = ' '.join(tokens_lemmatized)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T15:15:54.530233Z",
     "start_time": "2019-09-16T15:15:54.508691Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def searchkeywords(df, keyword, sort = 'Views', number = 5):\n",
    "    assert type(keyword) == str\n",
    "    df_keywords = df[df.Keywords_string.str.contains(keyword)]\n",
    "    if len(df_keywords)>0:\n",
    "        if sort=='Views' or sort=='Relvance':\n",
    "            df_keywords = df_keywords.sort_values(by= sort, ascending = False)\n",
    "        elif sort=='Upload Date':\n",
    "            df_keywords = df_keywords.sort_values(by= 'Timestamp', ascending = False)    \n",
    "        return df_keywords.iloc[0:number]\n",
    "    else:\n",
    "        return 'No Keywords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T16:04:08.259641Z",
     "start_time": "2019-09-15T16:04:08.255719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def checkoverlap(text, vectorizer):\n",
    "    text_list=text.split(' ')\n",
    "    vectorizer_names =vectorizer.get_feature_names()\n",
    "    count = 0\n",
    "    for word in text_list:\n",
    "        if word in vectorizer_names:\n",
    "            count+=1\n",
    "    if count > 0:\n",
    "        return True\n",
    "    else:\n",
    "        print('Please rephrase your input/wording.')\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T14:47:13.114878Z",
     "start_time": "2019-09-16T14:47:13.095771Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def recommendation(text, sort = 'Relevance', number = 5):\n",
    "#Applying Vectorization\n",
    "    tfidf = TfidfVectorizer(stop_words = stopwords1, max_df = 0.21, min_df = 0.02)\n",
    "    cv_mat = tfidf.fit_transform(df['Transcript_Description_lemmatized'])\n",
    "    nmf = NMF(100)\n",
    "    nmf_mat = nmf.fit_transform(cv_mat)\n",
    "    dist = pairwise_distances(nmf_mat)\n",
    "    recc=dist.argsort(axis=1)[:,0:number+1]\n",
    "    text = text.lower()\n",
    "    \n",
    "    link = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    if link != []:\n",
    "        link = link[0]\n",
    "        index = link.find('talks')\n",
    "        if index == -1:\n",
    "            print('This link is not a valid Ted Talk Link')\n",
    "            return\n",
    "        else:\n",
    "            link = link[index-1:]\n",
    "            index_link = list(df[df.Links == link].index)\n",
    "            recc1 = recc[index_link[0]][1:]\n",
    "            return recc1\n",
    "    elif len(text.split(' ')) <3:\n",
    "        recommendations_keywords = searchkeywords(df, keyword = text, sort = sort, number = number)\n",
    "        if type(recommendations_keywords) != str:\n",
    "            if len(recommendations_keywords) > 0 and len(recommendations_keywords)<number:\n",
    "                missing=number-len(recommendations_keywords)\n",
    "                top_recc = list(recommendations_keywords.index)\n",
    "                similar_docs=recc[top_recc[0]][1:]\n",
    "                for k in range(missing):\n",
    "                    top_recc.append(similar_docs[k])\n",
    "                return top_recc\n",
    "        \n",
    "            elif len(recommendations_keywords)==number:\n",
    "                return list(recommendations_keywords.index)\n",
    "        else:\n",
    "            #cleaned =cleaninput(text)\n",
    "            if checkoverlap(text,tfidf):\n",
    "                cv_new = tfidf.transform([text])\n",
    "            else:\n",
    "                return\n",
    "    else:\n",
    "        #cleaned = cleaninput(text)\n",
    "        if checkoverlap(text, tfidf):\n",
    "            cv_new = tfidf.transform([text])\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    nmf_new = nmf.transform(cv_new)\n",
    "    dist = pairwise_distances(nmf_new,nmf_mat)\n",
    "    recc=dist.argsort(axis=1)[:,0:number]\n",
    "    return recc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T16:05:00.089260Z",
     "start_time": "2019-09-15T16:04:26.045688Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = stopwords1, max_df = 0.21, min_df = 0.02)\n",
    "cv_mat = tfidf.fit_transform(df['Transcript_Description_lemmatized'])\n",
    "nmf = NMF(100)\n",
    "nmf_mat = nmf.fit_transform(cv_mat)\n",
    "dist = pairwise_distances(nmf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T19:43:09.322243Z",
     "start_time": "2019-09-15T19:42:59.593557Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(nmf_mat,open('~/Desktop/Projects/Project_5/Flask/data/Flask/NMF_Mat.pkl', 'wb'))\n",
    "pkl.dump(dist,open('~/Desktop/Projects/Project_5/Flask/data/Flask/Pairwise_dist.pkl', 'wb'))\n",
    "pkl.dump(nmf,open('~/Desktop/Projects/Project_5/Flask/data/Flask/NMF.pkl', 'wb'))\n",
    "df['Keywords_string']=df.Keywords.apply(lambda x: ' '.join(x))\n",
    "pkl.dump(df,open('~/Desktop/Projects/Project_5/Flask/data/Flask/df.pkl', 'wb'))\n",
    "pkl.dump(tfidf,open('~/Desktop/Projects/Project_5/Flask/data/Flask/tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T15:17:36.594850Z",
     "start_time": "2019-09-16T15:17:36.554330Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def recommendoutput(recommender, sort ='Relevance'):\n",
    "    \"\"\"\n",
    "    Converts the array given as output from the recommendation function to a\n",
    "    list of names of most similar articles.\n",
    "    \"\"\"\n",
    "    if recommender.any() ==None:\n",
    "        return 'No Recommendations'\n",
    "    else:\n",
    "        if len(recommender)==1:\n",
    "            recommender = recommender[0]\n",
    "        if sort=='Views':\n",
    "            df_temp=df[df.index.isin(recommender)].sort_values(by='Views', ascending=False)\n",
    "            recommender=list(df_temp.index)\n",
    "        elif sort=='Upload Date':\n",
    "            df_temp=df[df.index.isin(recommender)].sort_values(by='Timestamp', ascending=False)\n",
    "            recommender=list(df_temp.index)\n",
    "        name_list = [(df.Title.iloc[i], 'https://www.ted.com'+str(df.Links.iloc[i])) for i in recommender]\n",
    "        return name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T15:04:44.057758Z",
     "start_time": "2019-09-16T15:04:44.053157Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def recommendationsystem(text,sort='Relevance', number = 5):\n",
    "    recc = recommendation(text, number=number, sort=sort)\n",
    "    print(recc)\n",
    "    return recommendoutput(recc, sort=sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T17:13:50.981485Z",
     "start_time": "2019-09-09T17:13:50.911192Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Keywords_string']=df.Keywords.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:13:08.525032Z",
     "start_time": "2019-09-17T14:13:08.417332Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Generate wordclouds\n",
    "def wordcloud1(model, feature_names, no_top_words, topic):\n",
    "    labels=['Religion', 'Neuroscience', 'Solar System', 'Music', 'Medical Treatment','DNA/Genes', 'Economy',\n",
    "      'Astrophysics', 'Robotics', 'Politics','Oceanography', 'Education','Africa', 'Linguistics','War','Racial Discrimination',\n",
    "      'Art','Climate Change','Internet','Cancer','Transportation','Architecture/Design','Sex','Family','Artificial Intelligence',\n",
    "      'E-Sports','Asia','Virology','Children','Ecosystem','Recovery','Criminality']\n",
    "    index = labels.index(topic)\n",
    "    words = model.components_[index]\n",
    "    words1 = (\" \".join([feature_names[i] for i in words.argsort()[:-no_top_words - 1:-1]]))\n",
    "    cloud = WordCloud(background_color = \"white\", max_words = 200, stopwords = stopwords1)\n",
    "    cloud.generate(words1)\n",
    "    return cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:13:13.794580Z",
     "start_time": "2019-09-17T14:13:12.700377Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cloud = wordcloud1(nmf1, tfidf.get_feature_names(), 200, 'Ecosystem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:15:18.395790Z",
     "start_time": "2019-09-15T22:15:18.249308Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cloud.to_file('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Presentation/Wordclouds/Ecosystem_cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:12:50.396461Z",
     "start_time": "2019-09-17T14:12:50.084752Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.Topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T19:56:20.329250Z",
     "start_time": "2019-09-16T19:43:53.699004Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create String date in order to display in flask app\n",
    "df['Date_string']=df.Timestamp\n",
    "count =0\n",
    "for index,row in df.iterrows():\n",
    "    count+=1\n",
    "    print(count)\n",
    "    z=row.Timestamp\n",
    "    df['Date_string'].iloc[index]= str(z.month)+\"/\"+str(z.day)+\"/\"+str(z.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T20:07:47.487011Z",
     "start_time": "2019-09-16T20:07:36.470515Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(df,open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Flask/data/df.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
