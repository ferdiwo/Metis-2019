{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T15:59:44.723352Z",
     "start_time": "2019-09-13T15:59:44.718357Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:10:28.458774Z",
     "start_time": "2019-09-13T19:10:28.444303Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import os\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "requests.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T15:59:49.606639Z",
     "start_time": "2019-09-13T15:59:48.387957Z"
    }
   },
   "outputs": [],
   "source": [
    "#All Ted Talks\n",
    "url = 'https://www.ted.com/talks?page=73'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T15:59:49.619371Z",
     "start_time": "2019-09-13T15:59:49.610010Z"
    }
   },
   "outputs": [],
   "source": [
    "#BeautifulSoup works accordingly\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T15:59:50.380883Z",
     "start_time": "2019-09-13T15:59:50.375020Z"
    }
   },
   "outputs": [],
   "source": [
    "page = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T15:59:50.842794Z",
     "start_time": "2019-09-13T15:59:50.756978Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T15:59:51.397369Z",
     "start_time": "2019-09-13T15:59:51.372707Z"
    }
   },
   "outputs": [],
   "source": [
    "#Collect Links of all videos\n",
    "def scrapelinks():\n",
    "    links = []\n",
    "    broken = []\n",
    "    count = 0\n",
    "    for page in range(1,113):\n",
    "        url = 'https://www.ted.com/talks?page='+str(page)\n",
    "        response = requests.get(url) \n",
    "        soup = BeautifulSoup(response.text,'lxml')\n",
    "        x = soup.find_all('a',class_ = 'ga-link', attrs = {'data-ga-context':'talks'})\n",
    "        if len(x) == 0:\n",
    "            broken.append(page)\n",
    "        else:\n",
    "            for i in range(0,len(x),2):\n",
    "                count += 1\n",
    "                links.append(x[i]['href'])\n",
    "    return links, broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:02:26.388630Z",
     "start_time": "2019-09-13T15:59:54.061454Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weblinks, brokenpages = scrapelinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:56.918435Z",
     "start_time": "2019-09-13T16:03:56.902200Z"
    }
   },
   "outputs": [],
   "source": [
    "len(weblinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:02:26.417886Z",
     "start_time": "2019-09-13T16:02:26.405429Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrapebrokenpages(links, pages):\n",
    "    link = links\n",
    "    broken = []\n",
    "    for page in pages:\n",
    "        url = 'https://www.ted.com/talks?page='+str(page)\n",
    "        response = requests.get(url) \n",
    "        soup = BeautifulSoup(response.text,'lxml')\n",
    "        x = soup.find_all('a',class_ = 'ga-link', attrs = {'data-ga-context':'talks'})\n",
    "        if len(x) == 0:\n",
    "            broken.append(page)\n",
    "        for i in range(0,len(x),2):\n",
    "            link.append(x[i]['href'])\n",
    "    if broken != []:\n",
    "        scrapebrokenpages(link, broken)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:03.682625Z",
     "start_time": "2019-09-13T16:02:26.421759Z"
    }
   },
   "outputs": [],
   "source": [
    "links = scrapebrokenpages(weblinks, brokenpages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:03.769880Z",
     "start_time": "2019-09-13T16:03:03.744767Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrapeinfo(weblinks1, count1):\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    teddict = defaultdict(str)\n",
    "    title = []\n",
    "    views = []\n",
    "    videolength = []\n",
    "    videodescription = []\n",
    "    author = []\n",
    "    keywords = []\n",
    "    event = []\n",
    "    upload = []\n",
    "    authorjob = []\n",
    "    year = []\n",
    "    speakerdescription = []\n",
    "    rating = []\n",
    "    languages = []\n",
    "    comments = []\n",
    "    eventdate = []\n",
    "    count = count1\n",
    "    weblinks = weblinks1[count1:]\n",
    "    for link in weblinks:\n",
    "        count += 1\n",
    "        url = 'https://www.ted.com'+link\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        #Scrape Title\n",
    "        title.append(gettitle(soup))\n",
    "        #Scrape views\n",
    "        views.append(getviews(soup))\n",
    "        #Scrape videolength\n",
    "        videolength.append(getlength(soup))\n",
    "        #video description\n",
    "        videodescription.append(getdescription(soup))\n",
    "        #author\n",
    "        author.append(getauthor(soup))\n",
    "        #Keywords\n",
    "        keywords.append(getkeywords(soup))\n",
    "        #Event\n",
    "        event.append(getevent(soup))\n",
    "        #Upload \n",
    "        upload.append(getupload(soup))\n",
    "        #Job\n",
    "        authorjob.append(getjob(soup))\n",
    "        #Year\n",
    "        year.append(getyear(soup))\n",
    "        #Speaker Description\n",
    "        speakerdescription.append(getspeakerdescription(soup))\n",
    "        #Ratings\n",
    "        rating.append(getratings(soup))\n",
    "        #Number of languages\n",
    "        languages.append(getlanguages(soup))\n",
    "        #Number of Comments\n",
    "        comments.append(getcommentsnumber(soup))\n",
    "        #Event data\n",
    "        eventdate.append(geteventdate(soup))\n",
    "        print(count)\n",
    "        if count % 500 == 0:\n",
    "            teddict['Title'] = title\n",
    "            teddict['views'] = views\n",
    "            teddict['Length'] = videolength\n",
    "            teddict['Description'] = videodescription\n",
    "            teddict['Author'] = author\n",
    "            teddict['Keywords'] = keywords\n",
    "            teddict['Event'] = event\n",
    "            teddict['Upload Date'] = upload\n",
    "            teddict['Profession'] = authorjob\n",
    "            teddict['Year'] = year\n",
    "            teddict['Speaker Description'] = speakerdescription\n",
    "            teddict['Rating'] = rating\n",
    "            teddict['Languages'] = languages\n",
    "            teddict['Comments'] = comments\n",
    "            teddict['Event Date'] = eventdate\n",
    "            teddict['links'] = weblinks\n",
    "            df = pd.DataFrame(teddict, columns = ['Title', 'Views', 'Length', 'Description', 'Author', 'Keywords',\n",
    "                                                 'Event', 'Upload Date', 'Profession', 'Year', 'Speaker Description', \n",
    "                                                  'Rating', 'Languages', 'Comments', 'Event Date', 'Link'])\n",
    "            pkl.dump(df,open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Ted_info_'+str(count1)+'_'+str(count), 'wb'))\n",
    "        time.sleep(0.25)\n",
    "    \n",
    "    teddict['Title'] = title\n",
    "    teddict['Views'] = views\n",
    "    teddict['Length'] = videolength\n",
    "    teddict['Description'] = videodescription\n",
    "    teddict['Author'] = author\n",
    "    teddict['Keywords'] = keywords\n",
    "    teddict['Event'] = event\n",
    "    teddict['Upload Date'] = upload\n",
    "    teddict['Profession'] = authorjob\n",
    "    teddict['Year'] = year\n",
    "    teddict['Speaker Description'] = speakerdescription\n",
    "    teddict['Rating'] = rating\n",
    "    teddict['Languages'] = languages\n",
    "    teddict['Comments'] = comments\n",
    "    teddict['Event Date'] = eventdate\n",
    "    teddict['Links'] = weblinks\n",
    "    pkl.dump(teddict, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Ted_info_dict', 'wb'))\n",
    "    return teddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:10:28.439128Z",
     "start_time": "2019-09-13T16:06:01.080180Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = scrapeinfo(links, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:54:33.186605Z",
     "start_time": "2019-09-13T19:54:33.137884Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:54:34.056582Z",
     "start_time": "2019-09-13T19:54:34.038031Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(labels= ['title','views', 'length'], inplace= True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:54:34.610842Z",
     "start_time": "2019-09-13T19:54:34.569130Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:14:39.556547Z",
     "start_time": "2019-09-13T19:14:39.497525Z"
    }
   },
   "outputs": [],
   "source": [
    "#Videos as of Friday, September 13, 13:50 (4032 videos)\n",
    "pkl.dump(df, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:10.826446Z",
     "start_time": "2019-09-13T16:01:25.279Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T04:21:32.374305Z",
     "start_time": "2019-08-30T04:21:32.223543Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d, columns = ['Title', 'Views', 'Length', 'Description', 'Author', 'Keywords',\n",
    "                                                 'Event', 'Upload Date', 'Profession', 'Year', 'Speaker Description', \n",
    "                                                  'Rating', 'Languages', 'Comments', 'Event Date', 'Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T13:18:53.991375Z",
     "start_time": "2019-08-30T13:18:48.082780Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://www.ted.com/talks/iseult_gillespie_why_should_you_read_kafka_on_the_shore'\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(url)\n",
    "soup = BeautifulSoup(driver.page_source,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T13:18:54.167285Z",
     "start_time": "2019-08-30T13:18:53.997431Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T13:19:12.766470Z",
     "start_time": "2019-08-30T13:19:12.743221Z"
    }
   },
   "outputs": [],
   "source": [
    "getlength(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:31.351702Z",
     "start_time": "2019-09-13T16:03:31.335277Z"
    }
   },
   "outputs": [],
   "source": [
    "def getviews(soup):\n",
    "    \"\"\"\n",
    "    Scraping the number of views\n",
    "    \"\"\"\n",
    "    try:\n",
    "        number = soup.find('div', class_ = 'd:i-b f-w:700 f:.9 f:1@xxl').text\n",
    "        number_list = re.findall(r'\\d+',number)\n",
    "        if number.find(',') == -1:\n",
    "            numbers = number_list[0]\n",
    "        elif number.count(',') == 1:\n",
    "            numbers = ''.join(number_list[0:2])\n",
    "        else:\n",
    "            numbers = ''.join(number_list[0:3])\n",
    "    except:\n",
    "        numbers = np.NaN\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:34.537101Z",
     "start_time": "2019-09-13T16:03:34.519041Z"
    }
   },
   "outputs": [],
   "source": [
    "def getlength(soup):\n",
    "    \"\"\"\n",
    "    Scraping the length of the video\n",
    "    \"\"\"\n",
    "    try:\n",
    "        number = soup.find('div', class_ = 'd:i-b f-w:700 f:.9 f:1@xxl').text\n",
    "        number_list = list(map(int,re.findall(r'\\d+',number)))\n",
    "        if number.find(':') == -1:\n",
    "            numbers = number_list[-1]\n",
    "        elif number.count(':') == 1:\n",
    "            numbers = np.sum(number_list[-2])*60+number_list[-1]\n",
    "        else:\n",
    "            numbers = np.sum(number_list[-3])*(60**2)+np.sum(number_list[-2])*60+number_list[-1]\n",
    "    except:\n",
    "        numbers = np.NaN\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:36.387137Z",
     "start_time": "2019-09-13T16:03:36.378816Z"
    }
   },
   "outputs": [],
   "source": [
    "def getdescription(soup):\n",
    "    \"\"\"\n",
    "    Scraping the description of TED videos\n",
    "    \"\"\"\n",
    "    z = str(soup.find('meta', attrs = {'name':'description'}))\n",
    "    index1 = z.find('\"')\n",
    "    index2 = z.find('\"', index1+1)\n",
    "    description = z[index1+1:index2]\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:36.836562Z",
     "start_time": "2019-09-13T16:03:36.830309Z"
    }
   },
   "outputs": [],
   "source": [
    "def getauthor(soup):\n",
    "    \"\"\"\n",
    "    Scraping the author of the talk\n",
    "    \"\"\"\n",
    "    z = str(soup.find('meta', attrs = {'name':'author'}))\n",
    "    index1 = z.find('\"')\n",
    "    index2 = z.find('\"', index1+1)\n",
    "    author = z[index1+1:index2]\n",
    "    return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:37.173512Z",
     "start_time": "2019-09-13T16:03:37.162875Z"
    }
   },
   "outputs": [],
   "source": [
    "def getkeywords(soup):\n",
    "    \"\"\"\n",
    "    Scraping the keywords describing the video\n",
    "    \"\"\"\n",
    "    try:\n",
    "        z = str(soup.find('meta', attrs = {'name':'keywords'}))\n",
    "        index1 = z.find('\"')\n",
    "        index2 = z.find('\"', index1+1)\n",
    "        keywords = z[index1+1:index2]\n",
    "        keywords = keywords.split(',')\n",
    "    except:\n",
    "        keywords = np.NaN\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:37.453341Z",
     "start_time": "2019-09-13T16:03:37.439702Z"
    }
   },
   "outputs": [],
   "source": [
    "def getupload(soup):\n",
    "    \"\"\"\n",
    "    Scraping the upload date of the video\n",
    "    \"\"\"\n",
    "    z = str(soup.find('meta', attrs = {'itemprop':'uploadDate'}))\n",
    "    index1 = z.find('\"')\n",
    "    index2 = z.find('\"', index1+1)\n",
    "    date = z[index1+1:index2]\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:37.896023Z",
     "start_time": "2019-09-13T16:03:37.802234Z"
    }
   },
   "outputs": [],
   "source": [
    "def getevent(soup):\n",
    "    \"\"\"\n",
    "    Scraping the name of the event\n",
    "    \"\"\"\n",
    "    try:\n",
    "        h = str(soup.find_all('script', attrs = {'data-spec':\"q\"}))\n",
    "        start = h.find('event')\n",
    "        intermediate = h.find(\":\", start+1)\n",
    "        stop = h.find('\"',intermediate+2)\n",
    "        event = h[intermediate+2:stop]\n",
    "    except:\n",
    "        event = np.NaN\n",
    "    return event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:38.165653Z",
     "start_time": "2019-09-13T16:03:38.145170Z"
    }
   },
   "outputs": [],
   "source": [
    "def getjob(soup):\n",
    "    \"\"\"\n",
    "    Scraping the profession of the author\n",
    "    \"\"\"\n",
    "    try:\n",
    "        job = soup.find('span',  class_ = 'd:b d:i@md c:gray-d f:.9' ).text\n",
    "    except:\n",
    "        job = np.NaN\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:38.435539Z",
     "start_time": "2019-09-13T16:03:38.423958Z"
    }
   },
   "outputs": [],
   "source": [
    "def getyear(soup):\n",
    "    \"\"\"\n",
    "    Scraping the year of the talk\n",
    "    \"\"\"\n",
    "    try:\n",
    "        h = str(soup.find_all('script', attrs = {'data-spec':\"q\"}))\n",
    "        start = h.find('author')\n",
    "        intermediate = h.find('\"year\"', start)\n",
    "        stop = h.find(':',intermediate+2)\n",
    "        final = h.find('\"',stop+2)\n",
    "        year = h[stop+2:final]\n",
    "    except:\n",
    "        year = np.NaN\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:38.977236Z",
     "start_time": "2019-09-13T16:03:38.960313Z"
    }
   },
   "outputs": [],
   "source": [
    "def getspeakerdescription(soup):\n",
    "    \"\"\"\n",
    "    Scraping the description of the speaker\n",
    "    \"\"\"\n",
    "    try:\n",
    "        h = str(soup.find_all('script', attrs = {'data-spec':\"q\"}))\n",
    "        start = h.find('whotheyare')\n",
    "        stop = h.find(':',start)\n",
    "        final = h.find('\"',stop+2)\n",
    "        desc =h[stop+2:final]\n",
    "    except:\n",
    "        desc = np.NaN\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:39.283613Z",
     "start_time": "2019-09-13T16:03:39.256795Z"
    }
   },
   "outputs": [],
   "source": [
    "def getratings(soup):\n",
    "    \"\"\"\n",
    "    Scraping the ratings of the talk\n",
    "    \"\"\"\n",
    "    try:\n",
    "        h = str(soup.find_all('script', attrs = {'data-spec':\"q\"}))\n",
    "        start = h.find('ratings')\n",
    "        intermediate = h.find(\"[\", start)\n",
    "        stop = h.find(']',intermediate+1)\n",
    "        ratings = h[intermediate:stop+1].split(',')\n",
    "        d_list = []\n",
    "        for k in range(len(ratings)//3): \n",
    "            d = defaultdict(int)\n",
    "            if k == 0:\n",
    "                rating = ratings[0:3]\n",
    "            else:\n",
    "                rating = ratings[(k*3):(k*3+3)]\n",
    "            for i in rating:\n",
    "                start = i.find('\"')+1\n",
    "                stop = i.find('\"',start)\n",
    "                if i[start:stop] == 'name':\n",
    "                    start1 = i.find(':',stop)+2\n",
    "                    stop1 = i.find('\"',start1)\n",
    "                    d[i[start:stop]] = i[start1:stop1]\n",
    "                else:\n",
    "                    start1 = re.findall(r'\\d+', i)\n",
    "                    d[i[start:stop]] = int(start1[0])\n",
    "            d_list.append(d)\n",
    "    except:\n",
    "        d_list = np.NaN\n",
    "    return d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:39.645797Z",
     "start_time": "2019-09-13T16:03:39.633971Z"
    }
   },
   "outputs": [],
   "source": [
    "def getlanguages(soup):\n",
    "    \"\"\"\n",
    "    Scraping the number of languages the transcripts are available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = soup.find_all('span',class_ = \"css-i4xfca\")[1].get_text()\n",
    "        number = int(re.findall('\\d+', result)[0])\n",
    "    except:\n",
    "        number = np.NaN\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:40.944552Z",
     "start_time": "2019-09-13T16:03:40.939927Z"
    }
   },
   "outputs": [],
   "source": [
    "def gettitle(soup):\n",
    "    \"\"\"\n",
    "    Scraping the title of the video\n",
    "    \"\"\"\n",
    "    try:\n",
    "        z = str(soup.find('meta', attrs = {'name':'title'}))\n",
    "        index1 = z.find(':')\n",
    "        index2 = z.find('name=', index1)\n",
    "        title = z[index1+1:index2-2].strip()\n",
    "    except:\n",
    "        title = np.NaN\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:42.014985Z",
     "start_time": "2019-09-13T16:03:42.007244Z"
    }
   },
   "outputs": [],
   "source": [
    "def getcommentsnumber(soup):\n",
    "    \"\"\"\n",
    "    Scraping the number of comments\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = re.compile('Comments')\n",
    "        comments = soup.find(text=s)\n",
    "        number = int(re.findall('\\d+', comments)[0])\n",
    "    except:\n",
    "        number = np.NaN\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T16:03:42.728332Z",
     "start_time": "2019-09-13T16:03:42.720333Z"
    }
   },
   "outputs": [],
   "source": [
    "def geteventdate(soup):\n",
    "    \"\"\"\n",
    "    Scraping the month and year of the event\n",
    "    \"\"\"\n",
    "    try:\n",
    "        string = soup.find_all('span', class_ = 'f-w:700')[2].findNextSibling().text\n",
    "        start = string.find(' ')\n",
    "        date = string[start+1:]\n",
    "    except:\n",
    "        date = np.NaN\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T20:32:20.850913Z",
     "start_time": "2019-08-29T20:17:12.405Z"
    }
   },
   "source": [
    "## Transcript & Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:32:53.066821Z",
     "start_time": "2019-09-13T19:32:53.055848Z"
    }
   },
   "outputs": [],
   "source": [
    "def gettranscript(links, count1):\n",
    "    d_transcript = defaultdict(str)\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    count = count1\n",
    "    for link in links:\n",
    "        count += 1\n",
    "        try:\n",
    "            url = 'https://www.ted.com'+link+'/transcript'\n",
    "            driver.get(url)\n",
    "            soup = BeautifulSoup(driver.page_source,'lxml')   \n",
    "            transcript = ''\n",
    "            for i in soup.find_all('a', class_ ='t-d:n hover/bg:gray-l.5' ):\n",
    "                text1 = i.text.replace('\\n', '')\n",
    "                transcript = transcript + text1\n",
    "            d_transcript[link] = transcript\n",
    "        except:\n",
    "            continue\n",
    "        #if count % 500 == 0:\n",
    "            #pkl.dump(d_transcript, open('Transcripts_'+str(count1)+'_'+str(count), 'wb'))\n",
    "        print(count)\n",
    "    return d_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:24:33.210877Z",
     "start_time": "2019-09-13T19:24:33.197744Z"
    }
   },
   "outputs": [],
   "source": [
    "def getcomments(links, count1):\n",
    "    d_comments = defaultdict(str)\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    count = count1\n",
    "    for link in links:\n",
    "        count += 1\n",
    "        try:\n",
    "            url = 'https://www.ted.com'+link+'/discussion'\n",
    "            driver.get(url)\n",
    "            soup = BeautifulSoup(driver.page_source,'lxml')   \n",
    "            comments = []\n",
    "            for comment in soup.find_all('div', class_= 'comment__body hyphens'):\n",
    "                comments.append(comment.text.strip())\n",
    "            d_comments[link] = comments\n",
    "        except:\n",
    "            continue\n",
    "        if count%500 == 0:\n",
    "            pkl.dump(d_comments, open('~/Desktop/Projects/Project_5/Data/Comments_'+str(count1)+'_'+str(count), 'wb'))\n",
    "        print(count)\n",
    "    return d_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T17:04:19.940373Z",
     "start_time": "2019-08-30T15:16:03.090253Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_transcript = gettranscript(links, 0)\n",
    "pkl.dump(d_transcript, open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Transcript.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T17:21:09.070535Z",
     "start_time": "2019-08-30T17:04:20.003296Z"
    }
   },
   "outputs": [],
   "source": [
    "d_comments = getcomments(links, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T17:29:10.494284Z",
     "start_time": "2019-08-30T17:29:10.484111Z"
    }
   },
   "outputs": [],
   "source": [
    "len(d_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T17:21:15.394732Z",
     "start_time": "2019-08-30T17:21:15.387820Z"
    }
   },
   "outputs": [],
   "source": [
    "len(weblinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T17:21:16.514404Z",
     "start_time": "2019-08-30T17:21:16.504635Z"
    }
   },
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T07:58:42.065555Z",
     "start_time": "2019-09-03T07:58:41.900177Z"
    }
   },
   "outputs": [],
   "source": [
    "d = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Ted_info_dict', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:40:47.166415Z",
     "start_time": "2019-09-13T19:40:47.084611Z"
    }
   },
   "outputs": [],
   "source": [
    "d_transcript1 = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Transcripts.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T07:58:42.289540Z",
     "start_time": "2019-09-03T07:58:42.240884Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:54:57.131555Z",
     "start_time": "2019-09-13T19:54:57.121733Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Transcript'] = df['Links'].map(d_transcript1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:54:58.067550Z",
     "start_time": "2019-09-13T19:54:58.044479Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Transcript'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:55:01.064249Z",
     "start_time": "2019-09-13T19:55:00.746700Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Transcript']=df.Transcript.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:23:57.814452Z",
     "start_time": "2019-09-13T19:23:57.807430Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_transcripts=list(df[df.Transcript.isna()].Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:38:44.328813Z",
     "start_time": "2019-09-13T19:33:00.049621Z"
    }
   },
   "outputs": [],
   "source": [
    "d_transcript = gettranscript(missing_transcripts, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:55:08.778429Z",
     "start_time": "2019-09-13T19:55:08.769815Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Missing_Transcripts'] = df['Links'].map(d_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:55:10.314730Z",
     "start_time": "2019-09-13T19:55:10.292679Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_indexes=list(df[df.Transcript.isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:55:30.219915Z",
     "start_time": "2019-09-13T19:55:10.689536Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in missing_indexes:\n",
    "    df['Transcript'].iloc[index]=df['Missing_Transcripts'].iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:56:00.483298Z",
     "start_time": "2019-09-13T19:56:00.468869Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(labels=['Missing_Transcripts'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:56:26.322823Z",
     "start_time": "2019-09-13T19:56:26.154021Z"
    }
   },
   "outputs": [],
   "source": [
    "pkl.dump(df,open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Data+Transcripts.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T19:56:32.231023Z",
     "start_time": "2019-09-13T19:56:32.205668Z"
    }
   },
   "outputs": [],
   "source": [
    "d_comments = pkl.load(open('/Users/ferdinandwohlenberg/Desktop/Projects/Project_5/Data/Comments.pkl', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
