{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:46:11.217139Z",
     "start_time": "2019-08-04T17:46:04.311720Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89e5afb9ee61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Importing modules\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle as pkl\n",
    "from IPython import get_ipython\n",
    "#get_ipython().run_line_magic('pylab inline', 'config InlineBackend.figure_formats = ['retina']'\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#sns.set()\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, learning_curve, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, accuracy_score, f1_score, confusion_matrix, fbeta_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files from csv into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:46:11.221053Z",
     "start_time": "2019-08-04T17:46:05.111Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadfiles12():\n",
    "    \"\"\"\n",
    "    Load files into pandas DataFrame\n",
    "    Returns: DataFrames\n",
    "    \"\"\"\n",
    "    df_songs = pd.read_csv('/Desktop/Project_3/kkbox-music-recommendation-challenge/songs.csv', low_memory = True)\n",
    "    df_songs_extra = pd.read_csv('/Desktop/Project_3/kkbox-music-recommendation-challenge/song_extra_info.csv',low_memory = True)\n",
    "    df_members = pd.read_csv('/Desktop/Project_3/kkbox-music-recommendation-challenge/members.csv')\n",
    "    df_train = pd.read_csv('/Desktop/Project_3/kkbox-music-recommendation-challenge/train.csv')\n",
    "    df_test = pd.read_csv('/Desktop/Project_3/kkbox-music-recommendation-challenge/test.csv')\n",
    "\n",
    "    return df_songs, df_songs_extra, df_members, df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:46:11.222956Z",
     "start_time": "2019-08-04T17:46:05.967Z"
    }
   },
   "outputs": [],
   "source": [
    "def members_convrt(df_members):\n",
    "    \"\"\"\n",
    "    Parameter df_members: Members dataframe\n",
    "    Convert registration and expiration date to time Series.\n",
    "    Add new column with duration of the users membership\n",
    "    Convert 0's in 'bd' column (age) into the mean of people from the same area and same way of registration\n",
    "\n",
    "    Returns: Members DataFrame\n",
    "    \"\"\"\n",
    "    df_members['Registration'] = pd.to_datetime(df_members['registration_init_time'], format = '%Y%m%d')\n",
    "    df_members['Expiration Date'] = pd.to_datetime(df_members['expiration_date'], format = '%Y%m%d')\n",
    "    df_members.drop(columns = ['registration_init_time','expiration_date'], inplace = True)\n",
    "\n",
    "    #Compute registered timeframe\n",
    "    df_members['Registered Timeframe (days)'] = (df_members['Expiration Date'] - \\\n",
    "    df_members['Registration']).apply(lambda x: x.days)\n",
    "        \n",
    "\n",
    "    return df_members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:46:11.225026Z",
     "start_time": "2019-08-04T17:46:06.302Z"
    }
   },
   "outputs": [],
   "source": [
    "def genderconvert(df):\n",
    "    df['gender'].replace(to_replace = np.NaN, value = -1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:46:11.226755Z",
     "start_time": "2019-08-04T17:46:06.679Z"
    }
   },
   "outputs": [],
   "source": [
    "def bdconvert(df_members, df_train, df_songs):\n",
    "    \"\"\"\n",
    "    Convert 0's in 'bd' column (age) into the mean of people listening to same\n",
    "    genres and using same way of registration.\n",
    "    \"\"\"\n",
    "    df_members = df_members[['msno', 'bd','registered_via']].merge(df_train[['msno', 'song_id']], on = 'msno')\n",
    "    df_members = df_members.merge(df_songs[['song_id', 'genre_ids']], on = 'song_id')\n",
    "    age_mean = df_members[df_members.bd != 0].groupby(['registered_via', 'genre_ids'])\\\n",
    "    ['bd'].mean().reset_index()\n",
    "    age_dict = defaultdict(int)\n",
    "    for (c,r,a) in zip(age_mean['registered_via'], age_mean['genre_ids'], \\\n",
    "    age_mean['bd']):\n",
    "    #age_dict[c,r] = a\n",
    "        mask = (df_members.registered_via == c)&(df_members.genre_ids == r)&(df_members.bd == 0)\n",
    "        index_list = (df_members[mask].index)\n",
    "    for index in index_list:\n",
    "        df_members.loc[index,'bd'] = a\n",
    "    with open('msno_age.pkl',\"wb\")as file:\n",
    "        pkl.dump(df_members[['msno', 'bd']],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:46:11.229458Z",
     "start_time": "2019-08-04T17:46:06.983Z"
    }
   },
   "outputs": [],
   "source": [
    "#def registrationtoday(df_eda):\n",
    "    #timestamp = df_eda['Registration'].sort_values(ascending = False).iloc[0]\n",
    "    #df_eda['Registration_to_today'] = (pd.to_datetime(timestamp) -pd.to_datetime(df_eda['Registration'])).apply(lambda x: x.days)\n",
    "    #with open('msno_age.pkl',\"wb\")as file:\n",
    "        #pkl.dump(df_members[['msno', 'bd']],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active(df_eda):\n",
    "#Computing the time the user has been active in the music streaming service\n",
    "    timestamp = df_eda['Registration'].sort_values(ascending = False).iloc[0]\n",
    "    df_eda['Registration_to_today'] = (pd.to_datetime(timestamp) -pd.to_datetime(df_eda['Registration'])).apply(lambda x: x.days)\n",
    "    df_eda['Active Timeframe'] = df_eda['Registered Timeframe (days)']\n",
    "    df_eda['Active Timeframe'] = df_eda['Active Timeframe'][df_eda['Registered Timeframe (days)'] < df_eda['Registration_to_today']] = df_eda['Registered Timeframe (days)']\n",
    "    with open('msno_active_timeframe_eda.pkl',\"wb\")as file:\n",
    "        pkl.dump(df_eda[['msno', 'Registration', 'Registration_to_today', 'Active Timeframe']],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = pd.read_csv('/home/ubuntu/Project_3/df_eda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-adf226c95787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-5f70bb4eab91>\u001b[0m in \u001b[0;36mtimestamp\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_eda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_eda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdf_eda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Registration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'days_between_songs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'msno_timestamp.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msno'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Registration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Registration_to_today'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Active Timeframe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    655\u001b[0m             self.obj._data = self.obj._data.setitem(indexer=indexer,\n\u001b[1;32m    656\u001b[0m                                                     value=value)\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_align_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiindex_indexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_update_cacher\u001b[0;34m(self, clear, verify_is_copy)\u001b[0m\n\u001b[1;32m   3137\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3138\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3139\u001b[0;31m                     \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cache_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcacher\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3140\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_cache_changed\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   3094\u001b[0m         \"\"\"The object has called back to us saying maybe it has changed.\n\u001b[1;32m   3095\u001b[0m         \"\"\"\n\u001b[0;32m-> 3096\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mblk_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m                 \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 \u001b[0munfit_mgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, locs, values)\u001b[0m\n\u001b[1;32m   2696\u001b[0m         \"\"\"\n\u001b[1;32m   2697\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2698\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2699\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    \"\"\"\n",
    "    Add timestamp to each index\n",
    "    \"\"\"\n",
    "    df_eda = pkl.load(open( \"msno_active_timeframe_eda.pkl\", \"rb\" ))\n",
    "    df_eda['Timestamp'] = df_eda.Registration\n",
    "    df_eda['days_between_songs']=round((df_eda['Active Timeframe']/df_eda.groupby(['msno'])['msno'].transform('count')),0)\n",
    "    for index in df_eda.index:\n",
    "        count = len(df_eda.iloc[:index+1][df_eda.msno == df_eda.msno.iloc[index]])-1\n",
    "        df_eda['Timestamp'].iloc[index] = pd.to_datetime(df_eda['Registration'].iloc[index])+ datetime.timedelta(df_eda['days_between_songs'].iloc[index]*count)\n",
    "    with open('msno_timestamp.pkl',\"wb\")as file:\n",
    "        pkl.dump(df_eda[['msno', 'Registration', 'Registration_to_today', 'Active Timeframe']],file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrecvrt(df):\n",
    "    df_genre = df[df['genre_ids'].str.contains(pat = '\\|')]\n",
    "    df_genre1 = df_genre['genre_ids'].str.replace(re.compile('\\|\\d*'), repl ='')\n",
    "    df_genre['genre_ids'] = df_genre1\n",
    "    df.update(df_genre[['song_id', 'genre_ids']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrefix(df):\n",
    "    df['genre_ids'].fillna('-1', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artistcvrt(df):\n",
    "    df_artist = df[df['artist_name'].str.contains(pat = '\\|')]\n",
    "    df_artist1 = df_artist['artist_name'].str.replace(re.compile('\\|\\d*'), repl ='')\n",
    "    df_artist['artist_name'] = df_artist1\n",
    "    df.update(df_artist[['song_id', 'artist_name']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filllyricist(df):\n",
    "    df['lyricist'].fillna('no_lyricist',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillgenre(df):\n",
    "    df['genre_ids'].fillna('no_genreid', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillcomposer(df):\n",
    "    df['composer'].fillna('no_composer', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillartistname(df):\n",
    "    df['artist_name'].fillna('no_artist', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillsonglength(df):\n",
    "    df['song_length'].fillna(df['song_length'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillsongyear(df):\n",
    "    df['song_year'].replace(to_replace = np.NaN, value = int(df['song_year'].mean()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillsources(df):\n",
    "    df['source_system_tab'].fillna('-1', inplace = True)\n",
    "    df['source_screen_name'].fillna('-1', inplace = True)\n",
    "    df['source_type'].fillna('-1', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillnas(df):\n",
    "    filllyricist(df)\n",
    "    fillgenre(df)\n",
    "    fillcomposer(df)\n",
    "    fillartistname(df)\n",
    "    genrefix(df)\n",
    "    fillsources(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def songcategories(df):\n",
    "    df['song_length'] = df['song_length'].astype(np.uint32)\n",
    "    df['song_id'] = df['song_id'].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def languagecategories(df):\n",
    "    df['language'] = df['language'].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datawrangler(df):\n",
    "    \"\"\"\n",
    "    Applies Datawrangling functions\n",
    "    \"\"\"\n",
    "    df = artistcvrt(df)\n",
    "    fillnas(df)\n",
    "    df = genrecvrt(df)\n",
    "    df = songcategories(df)\n",
    "    df = languagecategories(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Call for Loading Data and Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs, df_songs_extra, df_members, df_train, df_test = loadfiles12()\n",
    "print('Loaded all files into DataFrames')\n",
    "\n",
    "df_members = members_convrt(df_members)\n",
    "print('Convert registration and expiration date to time Series.')\n",
    "\n",
    "df_members, df_members_dt = bdconvert(df_members, df_train, df_songs)\n",
    "print(\"Converted 0's in age columns\")\n",
    "\n",
    "create_eda_train_set(df_train, df_members, df_songs, df_songs_extra,1000)\n",
    "print('Saved eda_train_set as csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preparations for Modeling and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def splitdata(df,test_size = 0.2, seed = 89, plot = False ):\n",
    "    \"\"\"\n",
    "    Split data into train and validation set according to member id.\n",
    "\n",
    "    Parameter df: Dataframe to be split\n",
    "    Precondtion: df is a Pandas DataFrame\n",
    "\n",
    "    Parameter test_size: Size of test data set\n",
    "    Precondition: 0 ≤ test_size ≤ 1\n",
    "\n",
    "    Parameter seed: random number for random state generator\n",
    "    Precondition: type(seed) == int\n",
    "\n",
    "    Parameter identifier: Dataset feature according to which the dataset will be split\n",
    "    Precondition: identifier is a valid DataFrame index\n",
    "    \"\"\"\n",
    "    rs = np.random.RandomState(seed)\n",
    "    members_unique = df.msno.unique()\n",
    "    test_members = rs.choice(members_unique, size = int(members_unique.shape[0]*\\\n",
    "    test_size), replace = False)\n",
    "    df_tr = df[~df['msno'].isin(test_members)]\n",
    "    df_te = df[df['msno'].isin(test_members)]\n",
    "    \n",
    "    if plot:\n",
    "        sns.pairplot(df_tr)\n",
    "\n",
    "    y_tr, y_te = df_tr['target'], df_te['target']\n",
    "    X_tr = df_tr.drop(columns = ['target','msno'], axis = 1)\n",
    "    X_te = df_te.drop(columns = ['target','msno'], axis = 1)\n",
    "\n",
    "    return X_tr, X_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getdummyset(df):\n",
    "    X_dummies = pd.get_dummies(df.drop(columns = 'msno'),drop_first = True)\n",
    "    X_dummies['msno'] = df['msno']\n",
    "    X_train, X_test, y_train, y_test = splitdata(X_dummies, test_size = 0.2)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Scoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scoring(X_tr,X_te,y_tr,y_te ,model, model_name):\n",
    "    \"\"\"\n",
    "    Scoring baseline Model\n",
    "    \"\"\"\n",
    "    model.fit(X_tr,y_tr)\n",
    "    score = precision(y_te, model.predict(X_te))\n",
    "    print(model_name+' precision score:'+str(score))\n",
    "    cm = confusion_matrix(y_te, model.predict(X_te))\n",
    "    sns.heatmap(cm,\n",
    "                cmap=plt.cm.Blues,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "def score_baseline(df,columns, model,model_name, plot = False):\n",
    "    \"\"\"\n",
    "    AUC score of baseline\n",
    "    \"\"\"\n",
    "    columns_eda = columns\n",
    "    X_tr, X_te, y_tr, y_te = splitdata(df[columns_eda], test_size = 0.2, plot = plot)\n",
    "    model.fit(X_tr,y_tr)\n",
    "    score = roc_auc_score(y_te, model.predict(X_te))\n",
    "    print(model_name+' AUC score:'+str(score))\n",
    "    cm = confusion_matrix(y_te, model.predict(X_te))\n",
    "    sns.heatmap(cm,\n",
    "                cmap=plt.cm.Blues,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scoremodels(df, columns, models, model_name, plot = False):\n",
    "    \"\"\"\n",
    "    Scoring several models using the pre-defined score_baseline function and plotting pairplots\n",
    "    \"\"\"\n",
    "    for index, model in enumerate(models):\n",
    "        score_baseline(df,columns,model,model_name)\n",
    "        if plot:\n",
    "            plot_features(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def baselineanalysis():\n",
    "    df_eda = pd.read_csv('/home/ubuntu/Project_3/df_eda')\n",
    "    df_eda.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "    columns2 = ['msno','city', 'bd', 'registered_via','target','song_length']\n",
    "    model1 = LogisticRegression(C = 1)\n",
    "    model2 = KNeighborsClassifier(n_neighbors = 5)\n",
    "    model_name = ['Logistic Regression', 'KNN']\n",
    "    for index,model in enumerate([model1,model2]):\n",
    "        scoremodels(df_eda,columns2,[model], model_name[index], plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Evaluation metrics\n",
    "#add probabilitiess\n",
    "def accuracy(actuals, preds):\n",
    "    return np.mean(actuals == preds)\n",
    "\n",
    "def precision(actuals,preds):\n",
    "    tp = np.sum((actuals == 1) & (preds == 1))\n",
    "    fp = np.sum((actuals == 0) & (preds == 1))\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(actuals, preds):\n",
    "    tp = np.sum((actuals == 1) & (preds == 1))\n",
    "    fn = np.sum((actuals == 1) & (preds == 0))\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def F1(actuals, preds):\n",
    "    p, r = precision(actuals, preds), recall(actuals, preds)\n",
    "    return 2*p*r / (p + r)\n",
    "\n",
    "def f1_beta(actuals, preds, beta):\n",
    "    return fbeta_score(actuals, preds, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scoremodeldummy(df):\n",
    "    \"\"\"\n",
    "    Compute F1 Beta score for Logistic Regression, KNN, Random Forest and XGBoost.\n",
    "    Return predictions for each model\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = getdummyset(df)\n",
    "    #preds = getpredsdummy(KNeighborsClassifier(n_neighbors=5), X_train, X_test, y_train)\n",
    "    #preds2 = getpredsdummy(LogisticRegression(),X_train, X_test, y_train)\n",
    "    #preds3 = getpredsdummy(RandomForestClassifier(),X_train, X_test, y_train)\n",
    "    y_lgbm, y_lgbm_train, gbm = lightgbm(X_train, X_test, y_train, y_test)\n",
    "    #print('KNN AUC Score : '+str(roc_auc_score(y_test, preds)))\n",
    "    #print('Logistic AUC Score: '+str(roc_auc_score(y_test, preds2)))\n",
    "    #print('Random Forest AUC Score: '+str(roc_auc_score(y_test, preds3)))\n",
    "    print('Light GBM Regression Train AUC Score: '+str(roc_auc_score(y_train, y_lgbm_train)))\n",
    "    print('Light GBM Regression AUC Score: '+str(roc_auc_score(y_test, y_lgbm)))\n",
    "    return y_lgbm,y_lgbm_train, y_test, gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getpredsdummy(model, X_train, X_test, y_train):\n",
    "    model1 = model\n",
    "    model1.fit(X_train,y_train)\n",
    "    y_preds = model1.predict_proba(X_test)[:,1]\n",
    "    preds = y_preds > 0.5\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Fit KNN regression to training data and find optimal no. of neighbors\n",
    "def fitknn(df):\n",
    "    #Create dictionary to store values\n",
    "    X_train, X_test, y_train, y_test = getdummyset(df)\n",
    "    d_knn = defaultdict(int)\n",
    "    score = 0\n",
    "    k = 0\n",
    "    for i in range(1,50):\n",
    "        neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        preds = neigh.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        d_knn[str(i)] = accuracy\n",
    "        if accuracy > score:\n",
    "            score = accuracy\n",
    "            k = i\n",
    "    print('k value: '+str(k)+'\\n' \n",
    "          'accuracy: '+str(score))\n",
    "    return d_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Fit Logistics regression to training data and find optimal 'C' value\n",
    "def fitlog(df):\n",
    "    #Create dictionary to store values\n",
    "    d_logistic = defaultdict(int)\n",
    "    #Split dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = getdummyset(df)\n",
    "    score = 0\n",
    "    k = 1\n",
    "    for i in range(1,1002,100):\n",
    "        logistic = LogisticRegression(C = i)\n",
    "        logistic.fit(X_train, y_train)\n",
    "        preds = logistic.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        d_logistic[str(i)] = accuracy\n",
    "        if accuracy > score:\n",
    "            score = accuracy\n",
    "            k = i   \n",
    "    print('C value: '+str(k)+'\\n' \n",
    "          'accuracy: '+str(score))\n",
    "    return d_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Fit Decision Tree Classifier to training data\n",
    "def fitrf(df):\n",
    "    #Create dictionary to store values\n",
    "    X_train, X_test, y_train, y_test = getdummyset(df)\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    print('Random Forest Classifier accuracy: '+str(accuracy))\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def xgboost(X_train, X_test, y_train, y_test):\n",
    "    gbm = xgb.XGBClassifier(n_estimators=30000,\n",
    "                            max_depth=5,\n",
    "                            objective='binary:logistic',\n",
    "                            learning_rate=.05, \n",
    "                            subsample=.8,\n",
    "                            min_child_weight=2,\n",
    "                            colsample_bytree=.8)\n",
    "\n",
    "    eval_set=[(X_train,y_train),(X_test,y_test)]\n",
    "    fit_model = gbm.fit( \n",
    "                    X_train, y_train, \n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric='error', #new evaluation metric: classification error (could also use AUC, e.g.)\n",
    "                    early_stopping_rounds=40,\n",
    "                    verbose=False\n",
    "                   )\n",
    "\n",
    "    print(accuracy_score(y_test, gbm.predict(X_test, ntree_limit=gbm.best_ntree_limit))) \n",
    "    return gbm.predict(X_test, ntree_limit=gbm.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lightgbm(X_train, X_test, y_train, y_test):\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_error'},\n",
    "    'num_leaves': 32,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=40)\n",
    "    y_preds = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    y_preds1 = gbm.predict(X_train,num_iteration = gbm.best_iteration)\n",
    "    preds = y_preds >= 0.5\n",
    "    preds1 = y_preds1 >= 0.5\n",
    "    return preds, preds1, gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getpreds(df, model):\n",
    "    model1 = model\n",
    "    X_train, X_test, y_train, y_test = splitdata(df, test_size = 0.2)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_preds = model.predict_proba(X_test)[:,1]\n",
    "    preds = y_preds > 0.5025\n",
    "    \n",
    "    \n",
    "    return preds, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Visual Analysis Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_features(df, sample_size=1000):\n",
    "    \n",
    "    sample = (df.drop(['msno'],axis=1).sample(sample_size, random_state=44)) \n",
    "    sns.pairplot(sample,hue='target', plot_kws=dict(alpha=.3, edgecolor='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def confusionmatrix(y_lgr,y_knn,y_rf,y_xgboost, y_actual):\n",
    "    models = zip(range(1,5),\n",
    "             ['Logistic Regression', 'KNN','Random Forest', 'LightGBM'],\n",
    "             [y_lgr, y_knn, y_rf, y_xgboost])\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for ind, name, pred in models:\n",
    "        plt.subplot(2, 2, ind)\n",
    "        cm = confusion_matrix(y_actual,pred)\n",
    "        sns.heatmap(cm,\n",
    "                cmap=plt.cm.Blues,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')\n",
    "        plt.title(name, size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def confusionmatrix1(y_lgr, y_lgbm, y_actual):\n",
    "    models = zip(range(1,3),\n",
    "             ['Logistic Regression','LightGBM'],\n",
    "             [y_lgr, y_lgbm])\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for ind, name, pred in models:\n",
    "        plt.subplot(2, 2, ind)\n",
    "        cm = confusion_matrix(y_actual,pred)\n",
    "        sns.heatmap(cm,\n",
    "                cmap=plt.cm.Blues,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')\n",
    "        plt.title(name, size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create EDA Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eda_train_set(df_train, df_members,df_songs,df_songs_extra,entries):\n",
    "    \"\"\"\n",
    "    Creating training set for EDA Analysis with only a specific amount of data entries and\n",
    "    saving it as csv file.\n",
    "    \"\"\"\n",
    "    df_short = df_members.iloc[:entries,:]\n",
    "    member_ids = df_short.msno.unique()\n",
    "    df_train = df_train[df_train['msno'].isin(member_ids)]\n",
    "    df_train = df_train.merge(df_members, on = 'msno', how = 'left')\n",
    "    df_train = df_train.merge(df_songs, on = 'song_id')\n",
    "    df_train = df_train.merge(df_songs_extra, on = 'song_id')\n",
    "    df_train = registrationtoday(df_train)\n",
    "    df_train = active(df_train)\n",
    "    df_train = timestamp(df_train)\n",
    "    df_train.to_csv(path_or_buf = 'df_eda_new')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_members.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eda_train_set(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = pd.read_csv('/home/ubuntu/Project_3/df_eda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ['msno','city', 'bd', 'registered_via','target', 'song_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = datawrangler(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing F-1 score for Logistic Regression and KNN\n",
    "baselineanalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for recall, precision and accuracy of baseline \n",
    "preds_lr, y_test = getpreds(df_eda[baseline], LogisticRegression())\n",
    "recall_base = recall(y_test, preds_lr)\n",
    "precision_base = precision(y_test, preds_lr)\n",
    "accuracy_base = accuracy(y_test, preds_lr)\n",
    "f1beta = f1_beta(y_test, preds_lr, beta = 0.7)\n",
    "\n",
    "print('Recall Baseline score: '+ str(recall_base))\n",
    "print('Precision Baseline score: '+ str(precision_base))\n",
    "print('Accuracy Baseline score: '+ str(accuracy_base))\n",
    "print('F1 Beta Baseline score: '+ str(f1beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = df_eda['Registration'].sort_values(ascending = False).iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = df_eda['Registration'].sort_values(ascending = False).iloc[0]\n",
    "df_eda['Registration_to_today'] = (pd.to_datetime(timestamp) -pd.to_datetime(df_eda['Registration'])).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the time the user has been active in the music streaming service\n",
    "df_eda['Active Timeframe'] = df_eda['Registered Timeframe (days)']\n",
    "df_eda['Active Timeframe'] = df_eda['Active Timeframe'][df_eda['Registered Timeframe (days)'] < df_eda['Registration_to_today']] = df_eda['Registered Timeframe (days)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(df_eda):\n",
    "    for index in df_eda.index:\n",
    "        count = len(df_eda.iloc[:index+1][df_eda.msno == df_eda.msno.iloc[index]])-1\n",
    "        print(count)\n",
    "        df_eda['Timestamp'].iloc[index] = df_eda['Registration'].iloc[index]+ datetime.timedelta(df_eda['days_between_songs'].iloc[index]*count)\n",
    "    return df_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.iloc[:1][df_eda.msno == df_eda.msno.iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.iloc[:5][df_eda.msno =='uHqAtShXTRXju5GE8ri3ITsVFepPf8jUoCF7ffNOuqE=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_msno = defaultdict(int)\n",
    "for member in df_eda.msno.unique():\n",
    "    timestamp = pd.to_datetime((df_eda[df_eda.msno == member]['Registration'].iloc[0]))\n",
    "    d_msno[member] = df_eda[df_eda.msno == member].index\n",
    "    indexes = list(range(len(d_msno[member])))\n",
    "    for item in indexes:\n",
    "        mask = df_eda.msno == member\n",
    "        df_eda['Timestamp'][df_eda.msno == member].iloc[item] = timestamp + datetime.timedelta((df_eda[mask]['days_between_songs'].iloc[item])*item)\n",
    "\n",
    "for  member in d_msno:\n",
    "    indexes = d_msno[member[0]]\n",
    "    print(member)\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.rename(columns = {'Unnamed: 0':'index'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda[df_eda.msno == 'uHqAtShXTRXju5GE8ri3ITsVFepPf8jUoCF7ffNOuqE=']['Registration'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_msno = {'a':[1,3]}\n",
    "len(d_msno['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing F1 Beta\n",
    "X_train, X_test, y_train, y_test = splitdata(df_eda[baseline])\n",
    "y_knn,y_test = getpreds(df_eda[baseline],KNeighborsClassifier(n_neighbors=5))\n",
    "y_lm,y_test = getpreds(df_eda[baseline], LogisticRegression())\n",
    "y_rf,y_test = getpreds(df_eda[baseline], RandomForestClassifier())\n",
    "y_lgbm, y_lgbm_train = lightgbm(X_train, X_test, y_train, y_test)\n",
    "print('KNN F1 Beta: '+str(f1_beta(y_test, y_knn, 0.8)))\n",
    "print('Logistic F1 Beta: '+str(f1_beta(y_test, y_lm, 0.8)))\n",
    "print('Random Forest F1 Beta: '+str(f1_beta(y_test, y_rf, 0.8)))\n",
    "print('Light GBM F1 Beta: '+str(f1_beta(y_test, y_lgbm,0.8)))\n",
    "print('Light GBM F1 Beta Train Set: '+str(f1_beta(y_train, y_lgbm_train,0.8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Matrix for Baseline\n",
    "confusionmatrix(y_lm, y_knn,y_rf,y_lgbm,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Engineering Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def songlength_mean_msno(df):\n",
    "    df_mean = df.groupby(['msno'])['song_length'].mean().reset_index()\n",
    "    df1 = df.merge(df_mean, on = 'msno', how = 'left')\n",
    "    df1.rename(columns={\"song_length_y\": \"mean_song_length\"}, inplace = True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def artistcount(df):\n",
    "    df['count'] = df.groupby(['msno','artist_name'])['artist_name'].transform('count')\n",
    "    df.rename(columns={\"count\": \"artist_count\"}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def songpopularity (df):\n",
    "    \"\"\"\n",
    "    Counts the total number a song has been played\n",
    "    \"\"\"\n",
    "    dict_songs_played = {key: count for key, count in df['song_id'].value_counts().iteritems()}\n",
    "    df['Total_count_songs'] = df['song_id'].map(dict_songs_played)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def artistpopularity (df):\n",
    "    \"\"\"\n",
    "    Counts the total number an artist has been played\n",
    "    \"\"\"\n",
    "    dict_artist = {key: count for key, count in df['artist_name'].value_counts().iteritems()}\n",
    "    df['Total_count_artist'] = df['artist_name'].map(dict_artist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def composerpopularity(df):\n",
    "    \"\"\"\n",
    "    Counts the total number a composer has been played\n",
    "    \"\"\"\n",
    "    dict_artist = {key: count for key, count in df['composer'].value_counts().iteritems()}\n",
    "    df['Total_count_composer'] = df['composer'].map(dict_artist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lyricistpopularity(df):\n",
    "    \"\"\"\n",
    "    Counts the total number a lyricist has been played\n",
    "    \"\"\"\n",
    "    dict_artist = {key: count for key, count in df['lyricist'].value_counts().iteritems()}\n",
    "    df['Total_count_lyricist'] = df['lyricist'].map(dict_artist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def year_isrc(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convertisrc(df):\n",
    "    df['song_year'] = df['isrc'].apply(year_isrc)\n",
    "    fillsongyear(df)\n",
    "    df.drop(columns = 'isrc', inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def language_english(df):\n",
    "    \"\"\"Checks whether a song is in English or not\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def is_featured(artist):\n",
    "    if 'feat' in str(artist) :\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def features(df):\n",
    "    df['is_featured'] = df['artist_name'].apply(is_featured).astype(np.int8)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def shortsongapply(df):\n",
    "    \"\"\"\n",
    "    Checks whether a songis shorter than the songs the user is usually listening to\n",
    "    \"\"\"\n",
    "    list_songs = []\n",
    "    \n",
    "    df['short_song'] = np.where((df['song_length_x'] < df['mean_song_length']),1, 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sourceprobabilities(df):\n",
    "    \"\"\"\n",
    "    Computing the probability of a user using this specific source\n",
    "    \"\"\"\n",
    "    for source in ['source_system_tab', 'source_screen_name', 'source_type']:\n",
    "        df[source+'_msno_count'] =df.groupby(by = ['msno',source])[source].transform('count')\n",
    "        df[source+'_msno_count_total'] =df.groupby(by = ['msno'])['source_system_tab'].transform('count')\n",
    "        df['msno_'+source+'_probability'] = df[source+'_msno_count'] / df[source+'_msno_count_total']\n",
    "    \n",
    "    #Creating dict with overall probabilites of using source\n",
    "        total = df[source].count()\n",
    "        d_source = {key: count/total for key,count in df[source].value_counts().iteritems()}\n",
    "        df['total_'+source+'_probability'] = df[source].map(d_source)\n",
    "        df.drop(columns = source+'_msno_count_total')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def featureaddition(df):\n",
    "    df = songlength_mean_msno(df)\n",
    "    df = artistcount(df)\n",
    "    df = songpopularity(df)\n",
    "    df = artistpopularity(df)\n",
    "    df = composerpopularity(df)\n",
    "    df = lyricistpopularity(df)\n",
    "    df = convertisrc(df)\n",
    "    df = features(df)\n",
    "    df = shortsongapply(df)\n",
    "    df = sourceprobabilities(df)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Engineering Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_features = featureaddition(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Adding Registered Timeframe for user\n",
    "columns = ['msno','city', 'bd', 'song_length','registered_via', 'Registered Timeframe (days)', 'target']\n",
    "scoremodels(df_eda, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Adding average song length per user\n",
    "columns = ['msno','city', 'bd','song_length_x', 'registered_via','mean_song_length','target']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Adding number of times user has already listened to artist\n",
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Registered Timeframe (days)']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs', 'Total_count_artist']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs', 'Total_count_artist','song_year']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs', 'Total_count_artist','song_year']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression', plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Takes too long\n",
    "#def dummyartist(df, column):\n",
    "    #artists = df[column].unique()\n",
    "    #length = len(df)\n",
    "    #for artist in artists:\n",
    "        #df[artist] = np.zeros(length)\n",
    "        #df[df.artist_name == artist].replace(to_replace = 0, value = 1, inplace = True)\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderconvert(df_features)\n",
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs','genre_ids', 'Total_count_artist','Total_count_composer', 'Total_count_lyricist','song_year', 'language','source_system_tab',\n",
    "       'source_screen_name', 'source_type','gender','short_song','source_system_tab_msno_count', 'source_system_tab_msno_count_total',\n",
    "       'msno_source_system_tab_probability',\n",
    "       'total_source_system_tab_probability', 'source_screen_name_msno_count',\n",
    "       'source_screen_name_msno_count_total',\n",
    "       'msno_source_screen_name_probability',\n",
    "       'total_source_screen_name_probability', 'source_type_msno_count',\n",
    "       'source_type_msno_count_total', 'msno_source_type_probability',\n",
    "       'total_source_type_probability']#'language', 'name','composer', 'gender'\n",
    "preds2, y_lgbm, y_lgm_train, y_test = scoremodeldummy(df_features[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionmatrix1(preds2,y_lgbm,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_songs, df_songs_extra, df_members, df_train, df_test = loadfiles12()\n",
    "#print('Loaded all files into DataFrames')\n",
    "\n",
    "#df_members, df_member_dt = members_convrt(df_members)\n",
    "#print('Convert registration and expiration date to time Series.')\n",
    "#df_members = bdconvert(df_members, df_train, df_songs)\n",
    "#print(\"Converted 0's in age columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparetest(df): \n",
    "    df_train = df.merge(df_members, on = 'msno', how = 'left')\n",
    "    df_train= df_train.merge(df_songs, on = 'song_id')\n",
    "    df_train = df_train.merge(df_songs_extra, on = 'song_id')\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = preparetest(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.read_csv('/home/ubuntu/Project_3/kkbox-music-recommendation-challenge/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = datawrangler(df_train)\n",
    "#df_train= featureaddition(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genderconvert(df_train4)\n",
    "#columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs', 'Total_count_artist','Total_count_composer', 'Total_count_lyricist','song_year', 'language','gender','short_song','source_system_tab_msno_count', 'source_system_tab_msno_count_total']#'language', 'name','composer', 'gender','genre_ids'\n",
    "#y_lgbm, y_lgbm_train, y_test, gbm = scoremodeldummy(df_train4[columns])\n",
    "#'msno_source_system_tab_probability',\n",
    "       #'total_source_system_tab_probability', 'source_screen_name_msno_count',\n",
    "       #'source_screen_name_msno_count_total',\n",
    "       #'msno_source_screen_name_probability',\n",
    "       #'total_source_screen_name_probability', 'source_type_msno_count',\n",
    "       #'source_type_msno_count_total', 'msno_source_type_probability',\n",
    "       #'total_source_type_probability'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre1 = df_genre['genre_ids'].str.replace(re.compile('\\|\\d*'), repl ='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre['genre_ids'] = df_genre1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = genrecvrt(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda[df_eda['artist_name'].str.contains(pat = '\\|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing to pickle\n",
    "d ={} \n",
    "predictions = [preds, preds2, preds3, y_xgboost, y_test]\n",
    "for index ,model in enumerate(['lr', 'knn', 'rf', 'xgb', 'y_test']):\n",
    "    d[model] = predictions[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mvp_prediction.pkl',\"wb\")as file:\n",
    "    pkl.dump(d,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda[df_eda['genre_ids'].str.contains(pat = '\\|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.genre_ids.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
