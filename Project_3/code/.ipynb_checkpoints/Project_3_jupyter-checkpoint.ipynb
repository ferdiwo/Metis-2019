{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.226715Z",
     "start_time": "2019-08-07T18:27:57.166440Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing modules\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle as pkl\n",
    "from IPython import get_ipython\n",
    "#get_ipython().run_line_magic('pylab inline', 'config InlineBackend.figure_formats = ['retina']'\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#sns.set()\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, learning_curve, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, accuracy_score, f1_score, confusion_matrix, fbeta_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T18:04:31.354413Z",
     "start_time": "2019-08-04T18:04:31.351918Z"
    }
   },
   "source": [
    "### Reduce Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.244202Z",
     "start_time": "2019-08-07T18:27:58.228984Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if (props[col].dtype != object and props[col].dtype != 'datetime64[ns]'):  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \",col)\n",
    "            print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            print(\"dtype after: \",props[col].dtype)\n",
    "            print(\"******************************\")\n",
    "    \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Files from csv into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.259750Z",
     "start_time": "2019-08-07T18:27:58.246129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def loadfiles12():\n",
    "    \"\"\"\n",
    "    Load files into pandas DataFrame\n",
    "    Returns: DataFrames\n",
    "    \"\"\"\n",
    "    df_songs = pd.read_csv('/home/ubuntu/Project_3/kkbox-music-recommendation-challenge/songs.csv', low_memory = True)\n",
    "    df_songs_extra = pd.read_csv('/home/ubuntu/Project_3/kkbox-music-recommendation-challenge/song_extra_info.csv',low_memory = True)\n",
    "    df_members = pd.read_csv('/home/ubuntu/Project_3/kkbox-music-recommendation-challenge/members.csv', low_memory = True)\n",
    "    df_train = pd.read_csv('/home/ubuntu/Project_3/kkbox-music-recommendation-challenge/train.csv', low_memory = True)\n",
    "    df_test = pd.read_csv('/home/ubuntu/Project_3/kkbox-music-recommendation-challenge/test.csv', low_memory = True)\n",
    "\n",
    "    return df_songs, df_songs_extra, df_members, df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.282903Z",
     "start_time": "2019-08-07T18:27:58.261537Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def labelencoding(df):\n",
    "    for feature in ['msno', 'song_id', 'gender', 'language','Registration', 'Expiration Date',\n",
    "                    'genre_ids', 'artist_name', 'composer', 'lyricist', 'name', 'source_system_tab', 'source_screen_name','source_type']:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(df[feature])\n",
    "        df[feature] = le.transform(df[feature])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.303429Z",
     "start_time": "2019-08-07T18:27:58.284434Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def members_convrt(df_members):\n",
    "    \"\"\"\n",
    "    Parameter df_members: Members dataframe\n",
    "    Convert registration and expiration date to time Series.\n",
    "    Add new column with duration of the users membership\n",
    "    Convert 0's in 'bd' column (age) into the mean of people from the same area and same way of registration\n",
    "\n",
    "    Returns: Members DataFrame\n",
    "    \"\"\"\n",
    "    df_members['Registration'] = pd.to_datetime(df_members['registration_init_time'], format = '%Y%m%d')\n",
    "    df_members['Expiration Date'] = pd.to_datetime(df_members['expiration_date'], format = '%Y%m%d')\n",
    "    df_members.drop(columns = ['registration_init_time','expiration_date'], inplace = True)\n",
    "\n",
    "    #Compute registered timeframe\n",
    "    df_members['Registered Timeframe (days)'] = (df_members['Expiration Date'] - \\\n",
    "    df_members['Registration']).apply(lambda x: x.days)\n",
    "        \n",
    "\n",
    "    return df_members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.316511Z",
     "start_time": "2019-08-07T18:27:58.304985Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def genderconvert(df):\n",
    "    df['gender'].replace(to_replace = np.NaN, value = -1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.331674Z",
     "start_time": "2019-08-07T18:27:58.318095Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bdconvert(df_members, df_train, df_songs):\n",
    "    \"\"\"\n",
    "    Convert 0's in 'bd' column (age) into the mean of people listening to same\n",
    "    genres and using same way of registration.\n",
    "    \"\"\"\n",
    "    df_members = df_train[['msno', 'song_id']].merge(df_members, on = 'msno')\n",
    "    df_members = df_members.merge(df_songs[['song_id', 'genre_ids']], on = 'song_id')\n",
    "    age_mean = df_members[df_members.bd != 0].groupby(['registered_via', 'genre_ids'])\\\n",
    "    ['bd'].mean().reset_index()\n",
    "    age_dict = defaultdict(int)\n",
    "    for (c,r,a) in zip(age_mean['registered_via'], age_mean['genre_ids'], \\\n",
    "    age_mean['bd']):\n",
    "    #age_dict[c,r] = a\n",
    "        mask = (df_members.registered_via == c)&(df_members.genre_ids == r)&(df_members.bd == 0)\n",
    "        index_list = (df_members[mask].index)\n",
    "    for index in index_list:\n",
    "        df_members.loc[index,'bd'] = a\n",
    "    pkl.dump(df_members, open( \"members.pkl\", \"wb\" ))\n",
    "    #return df_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.351459Z",
     "start_time": "2019-08-07T18:27:58.334240Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#def registrationtoday(df_eda):\n",
    "    #timestamp = df_eda['Registration'].sort_values(ascending = False).iloc[0]\n",
    "    #df_eda['Registration_to_today'] = (pd.to_datetime(timestamp) -pd.to_datetime(df_eda['Registration'])).apply(lambda x: x.days)\n",
    "    #with open('msno_age.pkl',\"wb\")as file:\n",
    "        #pkl.dump(df_members[['msno', 'bd']],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.366648Z",
     "start_time": "2019-08-07T18:27:58.353392Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def active(df_eda):\n",
    "#Computing the time the user has been active in the music streaming service\n",
    "    timestamp = df_eda['Registration'].sort_values(ascending = False).iloc[0]\n",
    "    df_eda['Registration_to_today'] = (pd.to_datetime(timestamp) -pd.to_datetime(df_eda['Registration'])).apply(lambda x: x.days)\n",
    "    df_eda['Active Timeframe'] = df_eda['Registered Timeframe (days)']\n",
    "    df_eda['Active Timeframe'] = df_eda['Active Timeframe'][df_eda['Registered Timeframe (days)'] < df_eda['Registration_to_today']] = df_eda['Registered Timeframe (days)']\n",
    "    #with open('msno_active_timeframe_eda.pkl',\"wb\")as file:\n",
    "        #pkl.dump(df_eda[['msno', 'song_id','Registration', 'Registration_to_today', 'Active Timeframe']],file)\n",
    "    return df_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.381819Z",
     "start_time": "2019-08-07T18:27:58.368304Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def timestamp(df_eda):\n",
    "    \"\"\"\n",
    "    Add timestamp to each index\n",
    "    \"\"\"\n",
    "    df_eda = pkl.load(open( \"msno_active_timeframe_eda.pkl\", \"rb\" ))\n",
    "    df_eda['Timestamp'] = df_eda.Registration\n",
    "    df_eda['days_between_songs']=round((df_eda['Active Timeframe']/df_eda.groupby(['msno'])['msno'].transform('count')),0)\n",
    "    count1 = 0\n",
    "    for index in df_eda.index:\n",
    "        count1 +=1\n",
    "        if count1%2000 == 0:\n",
    "            print(count1)\n",
    "        count = len(df_eda.iloc[:index+1][df_eda.msno == df_eda.msno.iloc[index]])-1\n",
    "        df_eda['Timestamp'].iloc[index] = pd.to_datetime(df_eda['Registration'].iloc[index])+ datetime.timedelta(df_eda['days_between_songs'].iloc[index]*count)\n",
    "    with open('msno_timestamp.pkl',\"wb\")as file:\n",
    "        pkl.dump(df_eda,file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.401910Z",
     "start_time": "2019-08-07T18:27:58.383433Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#active(df_eda)\n",
    "#timestamp(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.415061Z",
     "start_time": "2019-08-07T18:27:58.403475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def genrecvrt(df):\n",
    "    df_genre = df[df['genre_ids'].str.contains(pat = '\\|')]\n",
    "    df_genre1 = df_genre['genre_ids'].str.replace(re.compile('\\|\\d*'), repl ='')\n",
    "    df_genre['genre_ids'] = df_genre1\n",
    "    df.update(df_genre[['song_id', 'genre_ids']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.428350Z",
     "start_time": "2019-08-07T18:27:58.416654Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def genrefix(df):\n",
    "    df['genre_ids'].fillna('-1', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.441594Z",
     "start_time": "2019-08-07T18:27:58.429948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def artistcvrt(df):\n",
    "    df_artist = df[df['artist_name'].str.contains(pat = '\\|')]\n",
    "    df_artist1 = df_artist['artist_name'].str.replace(re.compile('\\|\\d*'), repl ='')\n",
    "    df_artist['artist_name'] = df_artist1\n",
    "    df.update(df_artist[['song_id', 'artist_name']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.456126Z",
     "start_time": "2019-08-07T18:27:58.443252Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def filllyricist(df):\n",
    "    df['lyricist'].fillna('no_lyricist',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.476533Z",
     "start_time": "2019-08-07T18:27:58.457672Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fillgenre(df):\n",
    "    df['genre_ids'].fillna('no_genreid', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.489581Z",
     "start_time": "2019-08-07T18:27:58.478178Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fillcomposer(df):\n",
    "    df['composer'].fillna('no_composer', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.502503Z",
     "start_time": "2019-08-07T18:27:58.491142Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fillartistname(df):\n",
    "    df['artist_name'].fillna('no_artist', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.515490Z",
     "start_time": "2019-08-07T18:27:58.504019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fillsonglength(df):\n",
    "    df['song_length'].fillna(df['song_length'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.528392Z",
     "start_time": "2019-08-07T18:27:58.516986Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fillsongyear(df):\n",
    "    df['song_year'].replace(to_replace = np.NaN, value = int(df['song_year'].mean()), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.541539Z",
     "start_time": "2019-08-07T18:27:58.529945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fillsources(df):\n",
    "    df['source_system_tab'].fillna('-1', inplace = True)\n",
    "    df['source_screen_name'].fillna('-1', inplace = True)\n",
    "    df['source_type'].fillna('-1', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.554759Z",
     "start_time": "2019-08-07T18:27:58.543054Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fillbd(df):\n",
    "    df['bd'].fillna(-1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.568154Z",
     "start_time": "2019-08-07T18:27:58.558767Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fillnas(df):\n",
    "    filllyricist(df)\n",
    "    fillgenre(df)\n",
    "    fillcomposer(df)\n",
    "    fillartistname(df)\n",
    "    genrefix(df)\n",
    "    fillsources(df)\n",
    "    fillbd(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.581193Z",
     "start_time": "2019-08-07T18:27:58.570534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def songcategories(df):\n",
    "    df['song_length'] = df['song_length'].astype(np.uint32)\n",
    "    df['song_id'] = df['song_id'].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.594121Z",
     "start_time": "2019-08-07T18:27:58.582712Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def languagecategories(df):\n",
    "    df['language'] = df['language'].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.607302Z",
     "start_time": "2019-08-07T18:27:58.595664Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def datawrangler(df):\n",
    "    \"\"\"\n",
    "    Applies Datawrangling functions\n",
    "    \"\"\"\n",
    "    df = artistcvrt(df)\n",
    "    fillnas(df)\n",
    "    df = genrecvrt(df)\n",
    "    df = songcategories(df)\n",
    "    df = languagecategories(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function Call for Loading Data and Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.619887Z",
     "start_time": "2019-08-07T18:27:58.608890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df_songs, df_songs_extra, df_members, df_train, df_test = loadfiles12()\n",
    "#print('Loaded all files into DataFrames')\n",
    "#del df_test\n",
    "\n",
    "#df_members = members_convrt(df_members)\n",
    "#print('Convert registration and expiration date to time Series.')\n",
    "\n",
    "#df_members = bdconvert(df_members, df_train, df_songs)\n",
    "#print(\"Converted 0's in age columns\")\n",
    "\n",
    "#create_eda_train_set(df_train, df_members, df_songs, df_songs_extra,1000)\n",
    "#print('Saved eda_train_set as csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preparations for Modeling and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.634894Z",
     "start_time": "2019-08-07T18:27:58.621438Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def splitdata(df,test_size = 0.2, seed = 89, plot = False ):\n",
    "    \"\"\"\n",
    "    Split data into train and validation set according to member id.\n",
    "\n",
    "    Parameter df: Dataframe to be split\n",
    "    Precondtion: df is a Pandas DataFrame\n",
    "\n",
    "    Parameter test_size: Size of test data set\n",
    "    Precondition: 0 ≤ test_size ≤ 1\n",
    "\n",
    "    Parameter seed: random number for random state generator\n",
    "    Precondition: type(seed) == int\n",
    "\n",
    "    Parameter identifier: Dataset feature according to which the dataset will be split\n",
    "    Precondition: identifier is a valid DataFrame index\n",
    "    \"\"\"\n",
    "    rs = np.random.RandomState(seed)\n",
    "    members_unique = df.msno.unique()\n",
    "    test_members = rs.choice(members_unique, size = int(members_unique.shape[0]*\\\n",
    "    test_size), replace = False)\n",
    "    df_tr = df[~df['msno'].isin(test_members)]\n",
    "    df_te = df[df['msno'].isin(test_members)]\n",
    "    \n",
    "    if plot:\n",
    "        sns.pairplot(df_tr)\n",
    "\n",
    "    y_tr, y_te = df_tr['target'], df_te['target']\n",
    "    X_tr = df_tr.drop(columns = ['target','msno'], axis = 1)\n",
    "    X_te = df_te.drop(columns = ['target','msno'], axis = 1)\n",
    "\n",
    "    return X_tr, X_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.654658Z",
     "start_time": "2019-08-07T18:27:58.636409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getdummyset(df):\n",
    "    X_dummies = pd.get_dummies(df.drop(columns = 'msno'),drop_first = True)\n",
    "    X_dummies['msno'] = df['msno']\n",
    "    X_train, X_test, y_train, y_test = splitdata(X_dummies, test_size = 0.2)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Scoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.668089Z",
     "start_time": "2019-08-07T18:27:58.656197Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scoring(X_tr,X_te,y_tr,y_te ,model, model_name):\n",
    "    \"\"\"\n",
    "    Scoring baseline Model\n",
    "    \"\"\"\n",
    "    model.fit(X_tr,y_tr)\n",
    "    score = precision(y_te, model.predict(X_te))\n",
    "    print(model_name+' precision score:'+str(score))\n",
    "    cm = confusion_matrix(y_te, model.predict(X_te))\n",
    "    sns.heatmap(cm,\n",
    "                cmap=plt.cm.Blues,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.689282Z",
     "start_time": "2019-08-07T18:27:58.669657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "def score_baseline(df,columns, model,model_name, plot = False):\n",
    "    \"\"\"\n",
    "    AUC score of baseline\n",
    "    \"\"\"\n",
    "    columns_eda = columns\n",
    "    X_tr, X_te, y_tr, y_te = splitdata(df[columns_eda], test_size = 0.2, plot = plot)\n",
    "    model.fit(X_tr,y_tr)\n",
    "    score = roc_auc_score(y_te, model.predict(X_te))\n",
    "    print(model_name+' AUC score:'+str(score))\n",
    "    cm = confusion_matrix(y_te, model.predict(X_te))\n",
    "    sns.heatmap(cm,\n",
    "                cmap=plt.cm.Blues,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.709520Z",
     "start_time": "2019-08-07T18:27:58.690931Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scoremodels(df, columns, models, model_name, plot = False):\n",
    "    \"\"\"\n",
    "    Scoring several models using the pre-defined score_baseline function and plotting pairplots\n",
    "    \"\"\"\n",
    "    for index, model in enumerate(models):\n",
    "        score_baseline(df,columns,model,model_name)\n",
    "        if plot:\n",
    "            plot_features(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.722828Z",
     "start_time": "2019-08-07T18:27:58.711245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def baselineanalysis():\n",
    "    df_eda = pd.read_csv('/home/ubuntu/Project_3/df_eda')\n",
    "    df_eda.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "    columns2 = ['msno','city', 'bd', 'registered_via','target','song_length']\n",
    "    model1 = LogisticRegression(C = 1)\n",
    "    model2 = KNeighborsClassifier(n_neighbors = 5)\n",
    "    model_name = ['Logistic Regression', 'KNN']\n",
    "    for index,model in enumerate([model1,model2]):\n",
    "        scoremodels(df_eda,columns2,[model], model_name[index], plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.736242Z",
     "start_time": "2019-08-07T18:27:58.724479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Evaluation metrics\n",
    "#add probabilitiess\n",
    "def accuracy(actuals, preds):\n",
    "    return np.mean(actuals == preds)\n",
    "\n",
    "def precision(actuals,preds):\n",
    "    tp = np.sum((actuals == 1) & (preds == 1))\n",
    "    fp = np.sum((actuals == 0) & (preds == 1))\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(actuals, preds):\n",
    "    tp = np.sum((actuals == 1) & (preds == 1))\n",
    "    fn = np.sum((actuals == 1) & (preds == 0))\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def F1(actuals, preds):\n",
    "    p, r = precision(actuals, preds), recall(actuals, preds)\n",
    "    return 2*p*r / (p + r)\n",
    "\n",
    "def f1_beta(actuals, preds, beta):\n",
    "    return fbeta_score(actuals, preds, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:33:35.341073Z",
     "start_time": "2019-08-07T18:33:35.336360Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scoremodeldummy(df, columns, categorical):\n",
    "    \"\"\"\n",
    "    Compute F1 Beta score for Logistic Regression, KNN, Random Forest and XGBoost.\n",
    "    Return predictions for each model\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = getdummyset(df)\n",
    "    #preds = getpredsdummy(KNeighborsClassifier(n_neighbors=5), X_train, X_test, y_train)\n",
    "    #preds2 = getpredsdummy(LogisticRegression(),X_train, X_test, y_train)\n",
    "    #preds3 = getpredsdummy(RandomForestClassifier(),X_train, X_test, y_train)\n",
    "    y_lgbm, y_lgbm_train, gbm, df_importance = lightgbm(X_train, X_test, y_train, y_test, columns, categorical)\n",
    "    #print('KNN AUC Score : '+str(roc_auc_score(y_test, preds)))\n",
    "    #print('Logistic AUC Score: '+str(roc_auc_score(y_test, preds2)))\n",
    "    #print('Random Forest AUC Score: '+str(roc_auc_score(y_test, preds3)))\n",
    "    print('Light GBM Regression Train AUC Score: '+str(roc_auc_score(y_train, y_lgbm_train)))\n",
    "    print('Light GBM Regression AUC Score: '+str(roc_auc_score(y_test, y_lgbm)))\n",
    "    return y_lgbm,y_lgbm_train, y_test,X_test, gbm, df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.771549Z",
     "start_time": "2019-08-07T18:27:58.752818Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getpredsdummy(model, X_train, X_test, y_train):\n",
    "    model1 = model\n",
    "    model1.fit(X_train,y_train)\n",
    "    y_preds = model1.predict_proba(X_test)[:,1]\n",
    "    preds = y_preds > 0.5\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.785442Z",
     "start_time": "2019-08-07T18:27:58.773156Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Fit KNN regression to training data and find optimal no. of neighbors\n",
    "def fitknn(df):\n",
    "    #Create dictionary to store values\n",
    "    X_train, X_test, y_train, y_test = getdummyset(df)\n",
    "    d_knn = defaultdict(int)\n",
    "    score = 0\n",
    "    k = 0\n",
    "    for i in range(1,50):\n",
    "        neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        preds = neigh.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        d_knn[str(i)] = accuracy\n",
    "        if accuracy > score:\n",
    "            score = accuracy\n",
    "            k = i\n",
    "    print('k value: '+str(k)+'\\n' \n",
    "          'accuracy: '+str(score))\n",
    "    return d_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.800996Z",
     "start_time": "2019-08-07T18:27:58.786976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Fit Logistics regression to training data and find optimal 'C' value\n",
    "def fitlog(df):\n",
    "    #Create dictionary to store values\n",
    "    d_logistic = defaultdict(int)\n",
    "    #Split dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = getdummyset(df)\n",
    "    score = 0\n",
    "    k = 1\n",
    "    for i in range(1,1002,100):\n",
    "        logistic = LogisticRegression(C = i)\n",
    "        logistic.fit(X_train, y_train)\n",
    "        preds = logistic.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        d_logistic[str(i)] = accuracy\n",
    "        if accuracy > score:\n",
    "            score = accuracy\n",
    "            k = i   \n",
    "    print('C value: '+str(k)+'\\n' \n",
    "          'accuracy: '+str(score))\n",
    "    return d_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.821616Z",
     "start_time": "2019-08-07T18:27:58.802562Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Fit Decision Tree Classifier to training data\n",
    "def fitrf(df):\n",
    "    #Create dictionary to store values\n",
    "    X_train, X_test, y_train, y_test = getdummyset(df)\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    print('Random Forest Classifier accuracy: '+str(accuracy))\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.834971Z",
     "start_time": "2019-08-07T18:27:58.823159Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def xgboost(X_train, X_test, y_train, y_test):\n",
    "    gbm = xgb.XGBClassifier(n_estimators=30000,\n",
    "                            max_depth=5,\n",
    "                            objective='binary:logistic',\n",
    "                            learning_rate=.05, \n",
    "                            subsample=.8,\n",
    "                            min_child_weight=2,\n",
    "                            colsample_bytree=.8)\n",
    "\n",
    "    eval_set=[(X_train,y_train),(X_test,y_test)]\n",
    "    fit_model = gbm.fit( \n",
    "                    X_train, y_train, \n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric='error', #new evaluation metric: classification error (could also use AUC, e.g.)\n",
    "                    early_stopping_rounds=40,\n",
    "                    verbose=False\n",
    "                   )\n",
    "\n",
    "    print(accuracy_score(y_test, gbm.predict(X_test, ntree_limit=gbm.best_ntree_limit))) \n",
    "    return gbm.predict(X_test, ntree_limit=gbm.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.850179Z",
     "start_time": "2019-08-07T18:27:58.836546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lightgbm(X_train, X_test, y_train, y_test, columns, categorical):\n",
    "    lgb_train = lgb.Dataset(X_train, y_train, feature_name='auto',categorical_feature=categorical)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, feature_name='auto', categorical_feature=categorical)    \n",
    "    \n",
    "    params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 100,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=lgb_eval,early_stopping_rounds=50)\n",
    "    y_preds = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    y_preds1 = gbm.predict(X_train,num_iteration = gbm.best_iteration)\n",
    "    \n",
    "    df_imp = pd.DataFrame({'feature' : gbm.feature_name(),\n",
    "                      'importance' : gbm.feature_importance(importance_type='gain')})\n",
    "    return y_preds, y_preds1, gbm, df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.870470Z",
     "start_time": "2019-08-07T18:27:58.851697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getpreds(df, model):\n",
    "    model1 = model\n",
    "    X_train, X_test, y_train, y_test = splitdata(df, test_size = 0.2)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_preds = model.predict_proba(X_test)[:,1]\n",
    "    preds = y_preds > 0.5025\n",
    "    \n",
    "    \n",
    "    return y_preds, preds, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Visual Analysis Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.886042Z",
     "start_time": "2019-08-07T18:27:58.872093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_features(df, sample_size=1000):\n",
    "    \n",
    "    sample = (df.drop(['msno'],axis=1).sample(sample_size, random_state=44)) \n",
    "    sns.pairplot(sample,hue='target', plot_kws=dict(alpha=.3, edgecolor='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.906746Z",
     "start_time": "2019-08-07T18:27:58.887604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def confusionmatrix(y_lgr,y_knn,y_rf,y_xgboost, y_actual):\n",
    "    models = zip(range(1,5),\n",
    "             ['Logistic Regression', 'KNN','Random Forest', 'LightGBM'],\n",
    "             [y_lgr, y_knn, y_rf, y_xgboost])\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for ind, name, pred in models:\n",
    "        plt.subplot(2, 2, ind)\n",
    "        cm = confusion_matrix(y_actual,pred)\n",
    "        sns.heatmap(cm,\n",
    "                cmap=plt.cm.Blues,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')\n",
    "        plt.title(name, size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.920337Z",
     "start_time": "2019-08-07T18:27:58.908285Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def confusionmatrix1(y_lgr, y_lgbm, y_actual):\n",
    "    models = zip(range(1,3),\n",
    "             ['Logistic Regression','LightGBM'],\n",
    "             [y_lgr, y_lgbm])\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for ind, name, pred in models:\n",
    "        plt.subplot(2, 2, ind)\n",
    "        cm = confusion_matrix(y_actual,pred)\n",
    "        sns.heatmap(cm,\n",
    "                cmap=plt.cm.Blues,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')\n",
    "        plt.title(name, size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:31:38.809430Z",
     "start_time": "2019-08-07T18:31:34.367Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_ROC_curve(X_test, y_test, d_model):\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    for model in d_models.keys():\n",
    "        model1 = d_models[model]\n",
    "        fpr, tpr, _ = roc_curve(y_test,model1.predict_proba(X_test)[:,1])\n",
    "        score_auc = roc_auc_score(y_test,model1.predict_proba(X_test)[:,1])\n",
    "        ax.plot(fpr, tpr, label = model)\n",
    "        print(model+' AUC score: '+str(score_auc))\n",
    "        fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create EDA Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.934250Z",
     "start_time": "2019-08-07T18:27:58.921994Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_eda_train_set(df_train, df_members, df_songs, df_songs_extra,entries):\n",
    "    \"\"\"\n",
    "    Creating training set for EDA Analysis with only a specific amount of data entries and\n",
    "    saving it as csv file.\n",
    "    \"\"\"\n",
    "    df_short = df_members.iloc[:entries,:]\n",
    "    member_ids = df_short.msno.unique()\n",
    "    df_train = df_train[df_train['msno'].isin(member_ids)]\n",
    "    df_train = df_train.merge(df_short, on = 'msno', how = 'left')\n",
    "    df_train = df_train.merge(df_songs, on = 'song_id')\n",
    "    df_train = df_train.merge(df_songs_extra, on = 'song_id')\n",
    "    #df_train = registrationtoday(df_train)\n",
    "    #df_train = active(df_train)\n",
    "    #df_train = timestamp(df_train)\n",
    "    df_train.to_csv(path_or_buf = 'df_eda_new')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:27:58.951345Z",
     "start_time": "2019-08-07T18:27:58.935817Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_eda_train_set1(df_train,df_songs,df_songs_extra,entries):\n",
    "    \"\"\"\n",
    "    Creating training set for EDA Analysis with only a specific amount of data entries and\n",
    "    saving it as csv file.\n",
    "    \"\"\"\n",
    "    df_members = pkl.load(open('members.pkl', 'rb'))\n",
    "    df_short = df_members.iloc[:entries,:]\n",
    "    member_ids = df_short.msno.unique()\n",
    "    print('unique members')\n",
    "    df_train = df_train[df_train['msno'].isin(member_ids)]\n",
    "    df_train = df_train.merge(df_members, on = 'msno', how = 'left')\n",
    "    print('merged train members')\n",
    "    del df_members\n",
    "    df_train = df_train.merge(df_songs, on = 'song_id')\n",
    "    print('merged train songs')\n",
    "    del df_songs1   \n",
    "    df_train = df_train.merge(df_songs_extra, on = 'song_id')\n",
    "    #df_train = registrationtoday(df_train)\n",
    "    #df_train = active(df_train)\n",
    "    #df_train = timestamp(df_train)\n",
    "    df_train.to_csv(path_or_buf = 'df_eda_new')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Baseline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:00.170184Z",
     "start_time": "2019-08-07T18:27:58.952936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_eda = pd.read_csv('/home/ubuntu/Project_3/df_eda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:00.277803Z",
     "start_time": "2019-08-07T18:28:00.172077Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "df_eda = reduce_mem_usage(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:00.282470Z",
     "start_time": "2019-08-07T18:28:00.279692Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "baseline = ['msno','city', 'bd', 'registered_via','target', 'song_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:02.394041Z",
     "start_time": "2019-08-07T18:28:00.284072Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_eda = datawrangler(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:02.632260Z",
     "start_time": "2019-08-07T18:28:02.395927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df_eda = reduce_mem_usage(df_eda)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_eda['msno'])\n",
    "df_eda['msno'] = le.transform(df_eda['msno'])\n",
    "\n",
    "le1 = preprocessing.LabelEncoder()\n",
    "le1.fit(df_eda['song_id'])\n",
    "df_eda['song_id'] = le1.transform(df_eda['song_id'])\n",
    "\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2.fit(df_eda['language'])\n",
    "df_eda['language'] = le2.transform(df_eda['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:02.748814Z",
     "start_time": "2019-08-07T18:28:02.634254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_eda = reduce_mem_usage(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:26.030087Z",
     "start_time": "2019-08-07T18:28:02.750693Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Computing F-1 score for Logistic Regression and KNN\n",
    "baselineanalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:26.206078Z",
     "start_time": "2019-08-07T18:28:26.031842Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Check for recall, precision and accuracy of baseline \n",
    "y_preds_proba, preds_lr, y_test = getpreds(df_eda[baseline], LogisticRegression())\n",
    "recall_base = recall(y_test, preds_lr)\n",
    "precision_base = precision(y_test, preds_lr)\n",
    "accuracy_base = accuracy(y_test, preds_lr)\n",
    "f1beta = f1_beta(y_test, preds_lr, beta = 0.7)\n",
    "\n",
    "print('Recall Baseline score: '+ str(recall_base))\n",
    "print('Precision Baseline score: '+ str(precision_base))\n",
    "print('Accuracy Baseline score: '+ str(accuracy_base))\n",
    "print('F1 Beta Baseline score: '+ str(f1beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:30.909755Z",
     "start_time": "2019-08-07T18:28:26.207821Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Computing AUC Score\n",
    "X_train, X_test, y_train, y_test = splitdata(df_eda[baseline])\n",
    "y_knn_proba, y_knn,y_test = getpreds(df_eda[baseline],KNeighborsClassifier(n_neighbors=5))\n",
    "y_lm_proba, y_lm,y_test = getpreds(df_eda[baseline], LogisticRegression())\n",
    "y_rf_proba, y_rf,y_test = getpreds(df_eda[baseline], RandomForestClassifier())\n",
    "y_lgbm, y_lgbm_train, gbm, df_importance= lightgbm(X_train, X_test, y_train, y_test, baseline, categorical = [])\n",
    "print('KNN AUC: '+str(roc_auc_score(y_test, y_knn)))\n",
    "print('Logistic AUC: '+str(roc_auc_score(y_test, y_lm)))\n",
    "print('Random Forest AUC: '+str(roc_auc_score(y_test, y_rf)))\n",
    "print('Light GBM AUC: '+str(roc_auc_score(y_test,gbm.predict(X_test, num_iteration=gbm.best_iteration))))\n",
    "print('Light GBM AUC Train Set: '+str(roc_auc_score(y_train,gbm.predict(X_train, num_iteration=gbm.best_iteration))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:20.990551Z",
     "start_time": "2019-08-07T18:29:19.694083Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Classification Matrix for Baseline\n",
    "preds_lgbm = y_lgbm > 0.5\n",
    "confusionmatrix(y_lm, y_knn,y_rf,preds_lgbm,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:25.171563Z",
     "start_time": "2019-08-07T18:29:25.164352Z"
    }
   },
   "outputs": [],
   "source": [
    "def activetimeframe(df_eda):\n",
    "    timestamp = df_eda['Registration'].sort_values(ascending = False).iloc[0]\n",
    "    df_eda['Registration_to_today'] = (pd.to_datetime(timestamp) -pd.to_datetime(df_eda['Registration'])).apply(lambda x: x.days)\n",
    "    df_eda['Active Timeframe'] = df_eda['Registered Timeframe (days)']\n",
    "    df_eda['Active Timeframe'] = df_eda['Active Timeframe'][df_eda['Registered Timeframe (days)'] < df_eda['Registration_to_today']] = df_eda['Registered Timeframe (days)']\n",
    "    return df_eda\n",
    "    #with open('msno_active_timeframe_eda.pkl',\"wb\")as file:\n",
    "        #pkl.dump(df_eda[['msno', 'song_id','Registration', 'Registration_to_today', 'Active Timeframe']],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:25.374767Z",
     "start_time": "2019-08-07T18:29:25.367864Z"
    }
   },
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    \"\"\"\n",
    "    Add timestamp to each index\n",
    "    \"\"\"\n",
    "    df_members = pkl.load(open( \"msno_active_timeframe_eda.pkl\", \"rb\" ))\n",
    "    df_members['Timestamp'] = df_members.Registration\n",
    "    df_members['days_between_songs']=round((df_members['Active Timeframe']/df_members.groupby(['msno'])['msno'].transform('count')),0)\n",
    "    print('Days between songs have been added')\n",
    "    count1 = 0\n",
    "    for index in df_members.index:\n",
    "        count1 +=1\n",
    "        if count1%1000 == 0:\n",
    "            print(count1)\n",
    "        count = len(df_members.iloc[:index+1][df_members.msno == df_members.msno.iloc[index]])-1\n",
    "        df_members['Timestamp'].iloc[index] = pd.to_datetime(df_members['Registration'].iloc[index])+ datetime.timedelta(df_members['days_between_songs'].iloc[index]*count)\n",
    "    with open('msno_timestamp.pkl',\"wb\")as file:\n",
    "        pkl.dump(df_members[['msno', 'song_id', 'Timestamp']],file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:25.576594Z",
     "start_time": "2019-08-07T18:29:25.572667Z"
    }
   },
   "outputs": [],
   "source": [
    "def songlength_mean_msno(df):\n",
    "    df_mean = df.groupby(['msno'])['song_length'].mean().reset_index()\n",
    "    df1 = df.merge(df_mean, on = 'msno', how = 'left')\n",
    "    df1.rename(columns={\"song_length_y\": \"mean_song_length\"}, inplace = True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:25.920330Z",
     "start_time": "2019-08-07T18:29:25.916708Z"
    }
   },
   "outputs": [],
   "source": [
    "def artistcount(df):\n",
    "    df['count'] = df.groupby(['msno','artist_name'])['artist_name'].transform('count')\n",
    "    df.rename(columns={\"count\": \"artist_count\"}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:26.127134Z",
     "start_time": "2019-08-07T18:29:26.123344Z"
    }
   },
   "outputs": [],
   "source": [
    "def songpopularity (df):\n",
    "    \"\"\"\n",
    "    Counts the total number a song has been played\n",
    "    \"\"\"\n",
    "    dict_songs_played = {key: count for key, count in df['song_id'].value_counts().iteritems()}\n",
    "    df['Total_count_songs'] = df['song_id'].map(dict_songs_played)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:26.322475Z",
     "start_time": "2019-08-07T18:29:26.318678Z"
    }
   },
   "outputs": [],
   "source": [
    "def artistpopularity (df):\n",
    "    \"\"\"\n",
    "    Counts the total number an artist has been played\n",
    "    \"\"\"\n",
    "    dict_artist = {key: count for key, count in df['artist_name'].value_counts().iteritems()}\n",
    "    df['Total_count_artist'] = df['artist_name'].map(dict_artist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:26.512582Z",
     "start_time": "2019-08-07T18:29:26.508627Z"
    }
   },
   "outputs": [],
   "source": [
    "def composerpopularity(df):\n",
    "    \"\"\"\n",
    "    Counts the total number a composer has been played\n",
    "    \"\"\"\n",
    "    dict_artist = {key: count for key, count in df['composer'].value_counts().iteritems()}\n",
    "    df['Total_count_composer'] = df['composer'].map(dict_artist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:26.906682Z",
     "start_time": "2019-08-07T18:29:26.902854Z"
    }
   },
   "outputs": [],
   "source": [
    "def lyricistpopularity(df):\n",
    "    \"\"\"\n",
    "    Counts the total number a lyricist has been played\n",
    "    \"\"\"\n",
    "    dict_artist = {key: count for key, count in df['lyricist'].value_counts().iteritems()}\n",
    "    df['Total_count_lyricist'] = df['lyricist'].map(dict_artist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:27.178783Z",
     "start_time": "2019-08-07T18:29:27.174643Z"
    }
   },
   "outputs": [],
   "source": [
    "def year_isrc(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:27.436693Z",
     "start_time": "2019-08-07T18:29:27.433240Z"
    }
   },
   "outputs": [],
   "source": [
    "def convertisrc(df):\n",
    "    df['song_year'] = df['isrc'].apply(year_isrc)\n",
    "    fillsongyear(df)\n",
    "    df.drop(columns = 'isrc', inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:27.766771Z",
     "start_time": "2019-08-07T18:29:27.763966Z"
    }
   },
   "outputs": [],
   "source": [
    "def language_english(df):\n",
    "    \"\"\"Checks whether a song is in English or not\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:28.214844Z",
     "start_time": "2019-08-07T18:29:28.211772Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_featured(artist):\n",
    "    if 'feat' in str(artist) :\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:28.419145Z",
     "start_time": "2019-08-07T18:29:28.415816Z"
    }
   },
   "outputs": [],
   "source": [
    "def features(df):\n",
    "    df['is_featured'] = df['artist_name'].apply(is_featured).astype(np.int8)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:28.619568Z",
     "start_time": "2019-08-07T18:29:28.616208Z"
    }
   },
   "outputs": [],
   "source": [
    "def shortsongapply(df):\n",
    "    \"\"\"\n",
    "    Checks whether a songis shorter than the songs the user is usually listening to\n",
    "    \"\"\"\n",
    "    list_songs = []\n",
    "    \n",
    "    df['short_song'] = np.where((df['song_length_x'] < df['mean_song_length']),1, 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:28.816169Z",
     "start_time": "2019-08-07T18:29:28.810091Z"
    }
   },
   "outputs": [],
   "source": [
    "def sourceprobabilities(df):\n",
    "    \"\"\"\n",
    "    Computing the probability of a user using this specific source\n",
    "    \"\"\"\n",
    "    for source in ['source_system_tab', 'source_screen_name', 'source_type']:\n",
    "        df[source+'_msno_count'] =df.groupby(by = ['msno',source])[source].transform('count')\n",
    "        df[source+'_msno_count_total'] =df.groupby(by = ['msno'])['source_system_tab'].transform('count')\n",
    "        df['msno_'+source+'_probability'] = df[source+'_msno_count'] / df[source+'_msno_count_total']\n",
    "    \n",
    "    #Creating dict with overall probabilites of using source\n",
    "        total = df[source].count()\n",
    "        d_source = {key: count/total for key,count in df[source].value_counts().iteritems()}\n",
    "        df['total_'+source+'_probability'] = df[source].map(d_source)\n",
    "        df.drop(columns = source+'_msno_count_total')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:29.013811Z",
     "start_time": "2019-08-07T18:29:29.009266Z"
    }
   },
   "outputs": [],
   "source": [
    "def featureaddition(df):\n",
    "    df = activetimeframe(df)\n",
    "    df = songlength_mean_msno(df)\n",
    "    df = artistcount(df)\n",
    "    df = songpopularity(df)\n",
    "    df = artistpopularity(df)\n",
    "    df = composerpopularity(df)\n",
    "    df = lyricistpopularity(df)\n",
    "    df = convertisrc(df)\n",
    "    df = features(df)\n",
    "    df = shortsongapply(df)\n",
    "    df = sourceprobabilities(df)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:33.391876Z",
     "start_time": "2019-08-07T18:29:29.401059Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = featureaddition(df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:33.397751Z",
     "start_time": "2019-08-07T18:29:33.393724Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:33.906443Z",
     "start_time": "2019-08-07T18:29:33.399723Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adding Registered Timeframe for user\n",
    "columns = ['msno','city', 'bd', 'song_length','registered_via', 'Active Timeframe', 'target']\n",
    "scoremodels(df_eda, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:34.269661Z",
     "start_time": "2019-08-07T18:29:33.908192Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adding average song length per user\n",
    "columns = ['msno','city', 'bd','song_length_x', 'registered_via','mean_song_length','target']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:40.633352Z",
     "start_time": "2019-08-07T18:29:39.591939Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adding number of times user has already listened to artist\n",
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:41.734278Z",
     "start_time": "2019-08-07T18:29:40.635278Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Registered Timeframe (days)']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:42.735051Z",
     "start_time": "2019-08-07T18:29:41.736271Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:43.910608Z",
     "start_time": "2019-08-07T18:29:42.737000Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs', 'Total_count_artist']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:29:45.373837Z",
     "start_time": "2019-08-07T18:29:43.912378Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs', 'Total_count_artist','song_year']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.221675Z",
     "start_time": "2019-08-07T18:29:45.375899Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['msno','city', 'bd', 'song_length_x','registered_via','artist_count','target', 'mean_song_length', 'Total_count_songs', 'Total_count_artist','song_year']\n",
    "scoremodels(df_features, columns, [LogisticRegression()], 'Logistic Regression', plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.097945Z",
     "start_time": "2019-08-07T18:27:57.398Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = reduce_mem_usage(df_features)\n",
    "with open('feature_engineered.pkl',\"wb\")as file:\n",
    "        pkl.dump(df_features.drop(columns = 'Unnamed: 0'),file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:25.233418Z",
     "start_time": "2019-08-07T18:30:25.223569Z"
    }
   },
   "outputs": [],
   "source": [
    "del df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.100031Z",
     "start_time": "2019-08-07T18:27:57.403Z"
    }
   },
   "outputs": [],
   "source": [
    "#Takes too long\n",
    "#def dummyartist(df, column):\n",
    "    #artists = df[column].unique()\n",
    "    #length = len(df)\n",
    "    #for artist in artists:\n",
    "        #df[artist] = np.zeros(length)\n",
    "        #df[df.artist_name == artist].replace(to_replace = 0, value = 1, inplace = True)\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:42.279574Z",
     "start_time": "2019-08-07T18:30:41.945221Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = pkl.load(open('feature_engineered.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:42.563082Z",
     "start_time": "2019-08-07T18:30:42.281582Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = reduce_mem_usage(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:30:42.569076Z",
     "start_time": "2019-08-07T18:30:42.565057Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:41:28.208975Z",
     "start_time": "2019-08-07T18:41:28.204931Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['msno', 'song_id', 'target','city', 'gender','registered_via','bd',\n",
    "       'Registered Timeframe (days)', 'song_length_x', 'genre_ids', 'language','artist_name',\n",
    "       'Registration_to_today', 'Active Timeframe', 'mean_song_length',\n",
    "       'artist_count', 'Total_count_songs', 'Total_count_artist',\n",
    "       'Total_count_composer', 'Total_count_lyricist', 'song_year',\n",
    "       'is_featured', 'short_song', 'source_system_tab_msno_count',\n",
    "       'source_system_tab_msno_count_total',\n",
    "       'msno_source_system_tab_probability', 'total_source_system_tab_probability', 'source_screen_name_msno_count',\n",
    "       'source_screen_name_msno_count_total',\n",
    "       'msno_source_screen_name_probability',\n",
    "       'total_source_screen_name_probability', 'source_type_msno_count',\n",
    "       'source_type_msno_count_total', 'msno_source_type_probability',\n",
    "       'total_source_type_probability']\n",
    "#y_lgbm, y_lgbm_train, X_test, y_test, gbm = scoremodeldummy(df_features[columns], columns, categorical = ['song_id', 'language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:41:35.161551Z",
     "start_time": "2019-08-07T18:41:35.159214Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot_ROC_curve(X_test, y_test, [gbm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:41:35.414228Z",
     "start_time": "2019-08-07T18:41:35.411852Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusionmatrix1(preds2,y_lgbm,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:41:35.703644Z",
     "start_time": "2019-08-07T18:41:35.701181Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_songs, df_songs_extra, df_members, df_train, df_test = loadfiles12()\n",
    "#print('Loaded all files into DataFrames')\n",
    "\n",
    "#df_members, df_member_dt = members_convrt(df_members)\n",
    "#print('Convert registration and expiration date to time Series.')\n",
    "#df_members = bdconvert(df_members, df_train, df_songs)\n",
    "#print(\"Converted 0's in age columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T19:11:04.762842Z",
     "start_time": "2019-08-05T19:11:04.760437Z"
    }
   },
   "source": [
    "### Modeling Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.111434Z",
     "start_time": "2019-08-07T18:27:57.434Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pkl.load(open('train_complete', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.112658Z",
     "start_time": "2019-08-07T18:27:57.437Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.113675Z",
     "start_time": "2019-08-07T18:27:57.440Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = datawrangler(df_train)\n",
    "print('Finished datawrangling')\n",
    "df_train= featureaddition(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.114742Z",
     "start_time": "2019-08-07T18:27:57.442Z"
    }
   },
   "outputs": [],
   "source": [
    "pkl.dump(df_train, open('train_engineered', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.115808Z",
     "start_time": "2019-08-07T18:27:57.445Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pkl.load(open('train_engineered', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.116807Z",
     "start_time": "2019-08-07T18:27:57.447Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['song_id'] = df_train['song_id'].astype('object')\n",
    "df_train['gender'] = df_train['gender'].fillna('-1')\n",
    "df_train['language'] = df_train['language'].astype('object')\n",
    "df_train['language'].fillna('-1')\n",
    "df_train['name'] = df_train['name'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.117885Z",
     "start_time": "2019-08-07T18:27:57.452Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = labelencoding(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.119026Z",
     "start_time": "2019-08-07T18:27:57.455Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = reduce_mem_usage(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.120117Z",
     "start_time": "2019-08-07T18:27:57.458Z"
    }
   },
   "outputs": [],
   "source": [
    "pkl.dump(df_train,open('df_train_label', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:41:53.429244Z",
     "start_time": "2019-08-07T18:41:52.652136Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pkl.load(open('df_train_label','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:44:05.660337Z",
     "start_time": "2019-08-07T18:41:53.460373Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['msno', 'song_id', 'target', 'city', 'bd', 'gender', 'registered_via',\n",
    "       'Registration', 'Expiration Date',\n",
    "       'song_length_x', 'genre_ids', 'artist_name', 'composer', 'lyricist',\n",
    "       'language', 'name', 'Registration_to_today', 'Active Timeframe',\n",
    "       'mean_song_length', 'artist_count', 'Total_count_songs',\n",
    "       'Total_count_artist', 'Total_count_composer', 'Total_count_lyricist',\n",
    "       'song_year', 'is_featured', 'short_song',\n",
    "       'source_system_tab_msno_count', 'source_system_tab_msno_count_total',\n",
    "       'msno_source_system_tab_probability',\n",
    "       'total_source_system_tab_probability', 'source_screen_name_msno_count',\n",
    "       'source_screen_name_msno_count_total',\n",
    "       'msno_source_screen_name_probability',\n",
    "       'total_source_screen_name_probability', 'source_type_msno_count',\n",
    "       'source_type_msno_count_total', 'msno_source_type_probability',\n",
    "       'total_source_type_probability']#'language', 'name','composer', 'gender'\n",
    "categorical = ['song_id','language','Registration', 'Expiration Date',\n",
    "               'genre_ids','artist_name', 'composer', 'lyricist', 'name']\n",
    "X_train, X_test, y_train, y_test = splitdata(df_train[columns])\n",
    "y_lgbm, y_lgbm_train, gbm, df_importance  = lightgbm(X_train, X_test, y_train, y_test, columns, categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:44:08.794911Z",
     "start_time": "2019-08-07T18:44:05.662312Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Light GBM Regression Train AUC Score: '+str(roc_auc_score(y_train, y_lgbm_train)))\n",
    "print('Light GBM Regression AUC Score: '+str(roc_auc_score(y_test, y_lgbm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:44:08.802077Z",
     "start_time": "2019-08-07T18:44:08.797009Z"
    }
   },
   "outputs": [],
   "source": [
    "df_importance.to_csv('/home/ubuntu/Project_3/Feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:44:13.518158Z",
     "start_time": "2019-08-07T18:44:08.803821Z"
    }
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "preds = y_lgbm > 0.58\n",
    "plt.figure(figsize=(10,6))\n",
    "cm = confusion_matrix(y_test,preds)\n",
    "sns.heatmap(cm,cmap=plt.cm.Blues,annot=True,square=True,\n",
    "                xticklabels=[0,1],\n",
    "                yticklabels=[0,1],\n",
    "                fmt='g')\n",
    "plt.title('Light GBM Confusion Matrix', size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:44:27.408558Z",
     "start_time": "2019-08-07T18:44:13.520026Z"
    }
   },
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "plt.figure(figsize=(10,6))\n",
    "fpr, tpr, _ = roc_curve(y_test,gbm.predict(X_test, num_iteration=gbm.best_iteration))\n",
    "score_auc = roc_auc_score(y_test,gbm.predict(X_test, num_iteration=gbm.best_iteration))\n",
    "plt.plot(fpr, tpr, label = 'Light GBM ROC Curve')\n",
    "print(score_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.127455Z",
     "start_time": "2019-08-07T18:27:57.479Z"
    }
   },
   "outputs": [],
   "source": [
    "df_roc_auc = pd.DataFrame(data = fpr, columns = ['False Positive Rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.128502Z",
     "start_time": "2019-08-07T18:27:57.481Z"
    }
   },
   "outputs": [],
   "source": [
    "df_roc_auc['True Positive Rate'] = tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:28:32.129482Z",
     "start_time": "2019-08-07T18:27:57.485Z"
    }
   },
   "outputs": [],
   "source": [
    "df_roc_auc.to_csv('ROC_AUC_Scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T18:44:27.593101Z",
     "start_time": "2019-08-07T18:44:27.410304Z"
    }
   },
   "outputs": [],
   "source": [
    "#Recall, Precision, Accuracy, F1\n",
    "actuals = y_test\n",
    "preds = y_lgbm > 0.58\n",
    "accuracy_lgbm = accuracy(y_test, preds)\n",
    "precision_lgbm = precision(y_test, preds)\n",
    "recall_lgbm = recall(y_test, preds)\n",
    "f1 = F1(y_test, preds)\n",
    "\n",
    "print('Recall Light GBM score: '+ str(recall_lgbm))\n",
    "print('Precision Light GBM score: '+ str(precision_lgbm))\n",
    "print('Accuracy Light GBM score: '+ str(accuracy_lgbm))\n",
    "print('F1 Light GBM score: '+ str(f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
